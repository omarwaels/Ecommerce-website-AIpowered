{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab85417a",
   "metadata": {},
   "source": [
    "# top n reccommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edd5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from ast import literal_eval\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\allData.csv'\n",
    "ratingPath = 'C:\\\\Users\\\\Abdo\\\\Desktop\\\\both3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1242db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_indices( product_indices):\n",
    "    fig, axs = plt.subplots(1, 10,figsize=(18,4))\n",
    "    fig.suptitle('You may also like these products', size = 22)\n",
    "    i = 0\n",
    "    for ind in product_indices:\n",
    "        response = requests.get(literal_eval(df2['imageURLHighRes'][ind])[0])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].set_title('{}  '.format(round(df2['vote_average'][ind],2)),y=-0.18,color=\"red\",fontsize=18)\n",
    "        i = i+1\n",
    "        fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_asin(asin):\n",
    "    response = requests.get(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "    plt.title(label= '{}  '.format(round(df2[df2['asin'] == asin]['vote_average'].values[0],2)) ,\n",
    "          loc=\"right\",\n",
    "          color='red' , fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv( dataPath)\n",
    "df3= pd.read_csv( ratingPath)\n",
    "df2['vote_average'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.userId.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "C= df2['vote_average'].mean()\n",
    "m= df2['vote_count'].quantile(0.9)\n",
    "df22 = df2.copy().loc[df2['vote_count'] >= m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e5f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFramePhone = df22[df22['category'].str[2:27] == 'Cell Phones & Accessories']\n",
    "\n",
    "dataFrameAppliances = df22[df22['category'].str[2:12] == 'Appliances']\n",
    "dataFrameSports = df22[df22['category'].str[2:19] == 'Sports & Outdoors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFramePhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a5047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFramePhone['score'] = dataFramePhone.apply(weighted_rating, axis=1)\n",
    "\n",
    "dataFrameAppliances['score'] = dataFrameAppliances.apply(weighted_rating, axis=1)\n",
    "dataFrameSports['score'] = dataFrameSports.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b839c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFramePhone = dataFramePhone.sort_values('score', ascending=False)\n",
    "dataFramePhone[['title', 'vote_count', 'vote_average', 'score']].head(10)\n",
    "# arr1 = np.array(dataFramePhone['asin'][0:15])\n",
    "# arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91717a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFramePhone['asin'][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameAppliances = dataFrameAppliances.sort_values('score', ascending=False)\n",
    "dataFrameAppliances[['title', 'vote_count', 'vote_average', 'score']].head(10)\n",
    "# arr2 = np.array(dataFrameAppliances['asin'][0:15])\n",
    "# arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa65111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameSports = dataFrameSports.sort_values('score', ascending=False)\n",
    "dataFrameSports[['title', 'vote_count', 'vote_average', 'score']].head(10)\n",
    "#arr3 = np.array(dataFrameSports['asin'][0:15])\n",
    "#arr3\n",
    "#coldstart = np.concatenate((arr1 , arr2 , arr3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdbf65",
   "metadata": {},
   "source": [
    "# content-based on feature and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb17a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english') # playing--->play\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "tfidf = TfidfVectorizer( analyzer = stemmed_words ,\n",
    "                     ngram_range=(1,1),\n",
    "                     min_df=3/31858, # if the word is repeated less than three times then I dont care about it\n",
    "                     max_df=0.5, # if the word is repeated more than 50% then I don't care about it\n",
    "                     stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(  df2['description'] + \" \" + df2['feature'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3acdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a99934",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim1 = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim1.pkl','wb') as f:\n",
    "    pickle.dump(cosine_sim1, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim1.pkl','rb') as f:\n",
    "    cosine_sim1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4018f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices1 = pd.Series(df2.index, index=df2['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(asin, cosine_sim , indices):\n",
    "    \n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # list of tuples [(0,1) , (1,0.5)]\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11]  # don't take 0 as we don't take the score with the same product\n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86682a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('B0006TIA8Y' , cosine_sim1  ,indices1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2894e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(df2['brand']+\" \"+ df2['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38786dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a97fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim2 = cosine_similarity(count_matrix,count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9249c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim2.pkl','wb') as f:\n",
    "    pickle.dump(cosine_sim2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim2.pkl','rb') as f:\n",
    "    cosine_sim2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices2 = pd.Series(df2.index, index=df2['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3713ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations( 'B0006TIA8Y', cosine_sim2 , indices2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the input is new to the dataset and not yet calculate its cosine with other products\n",
    "def tf_idftransformer (asin) :\n",
    "    x = tfidf.transform(  df2[df2['asin']== asin]['description'] + \" \" +df2[df2['asin']== asin]['feature'] )\n",
    "    cosine_sim = cosine_similarity(x, tfidf_matrix)\n",
    "    #cosine_sim1 = cosine_sim1 + cosine_sim  \n",
    "    indices = pd.Series(df2.index, index=df2['asin'])\n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idftransformer('B0006TIA8Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transformer (asin) :\n",
    "    x = count.transform(  df2[df2['asin']== asin]['brand'] + \" \" +df2[df2['asin']== asin]['category'] )\n",
    "    cosine_sim = cosine_similarity(x, count_matrix)\n",
    "    indices = pd.Series(df2.index, index=df2['asin'])\n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_transformer('B0006TIA8Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fa07e",
   "metadata": {},
   "source": [
    "# collaborative filtering with svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a69b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from surprise import Reader, Dataset, SVD , dump\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import os\n",
    "import random\n",
    "from math import log2\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision_recall_at_k(predictions, k=10, threshold=3):\n",
    "#     \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "#     # First map the predictions to each user.\n",
    "#     user_est_true = defaultdict(list)\n",
    "#     for uid, asin, true_r, est, _ in predictions:\n",
    "#         user_est_true[uid].append((est, true_r,asin))\n",
    "\n",
    "#     precisions = dict()\n",
    "#     recalls = dict()\n",
    "#     hitrate = dict()\n",
    "#     for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "#         # Sort user ratings by estimated value\n",
    "#         user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "#         # Number of relevant items\n",
    "#         n_rel = sum((true_r >= threshold) for (_, true_r,_) in user_ratings)\n",
    "\n",
    "#         # Number of recommended items in top k\n",
    "#         n_rec_k = sum((est >= threshold) for (est, _,_) in user_ratings[:k])\n",
    "\n",
    "#         # Number of relevant and recommended items in top k\n",
    "#         n_rel_and_rec_k = sum(\n",
    "#             ((true_r >= threshold) and (est >= threshold))\n",
    "#             for (est, true_r,_) in user_ratings[:k]\n",
    "#         )\n",
    "\n",
    "#         # Precision@K: Proportion of recommended items that are relevant\n",
    "#         # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "#         precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "#         # Recall@K: Proportion of relevant items that are recommended\n",
    "#         # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "#         recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        \n",
    "#         hitrate[uid] = 1 if (_,_,asin) in in user  \n",
    "\n",
    "#     return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision_recall_at_k(predictions, model,k=10, threshold=3 ):\n",
    "    \n",
    "#     recalls = dict()\n",
    "#     hit = dict()\n",
    "#     actual = defaultdict(list)\n",
    "#     asins =[]\n",
    "#     users=[]\n",
    "#     df = pd.DataFrame(predictions, columns = ['a','b','c' , 'd' ,'e'])\n",
    "    \n",
    "#     for uid, asin, true_r, est, _ in predictions:\n",
    "#         users.append(uid)\n",
    "#         asins.append(asin)            \n",
    "#         if (true_r >= threshold):\n",
    "#             actual[uid].append(asin)\n",
    "            \n",
    "#     asins = list(set(asins))\n",
    "#     users = set(users)\n",
    "#     i=1\n",
    "#     for user in users:\n",
    "        \n",
    "#         estimations = []\n",
    "#         predicted = []\n",
    "            \n",
    "                    \n",
    "#         if model =='neumf':\n",
    "#             user1 = [user for i in range(len(asins))]\n",
    "#             e = model2.predict([np.array(user1),np.array(asins)],verbose=False).T[0]\n",
    "#             sort = np.argsort(e)\n",
    "#             estimations = np.array(asins)[sort]\n",
    "#             predicted = [asin for asin in estimations[:k] if  float(e[sort][np.where(estimations == asin)[0][0]]) >= threshold]\n",
    "        \n",
    "        \n",
    "#         if model ==\"svd\":\n",
    "#             for asin in asins:\n",
    "#                 e = svd.predict(user,asin).est\n",
    "#                 estimations.append((asin ,e ))\n",
    "                \n",
    "#             estimations.sort(key=lambda x: x[1], reverse=True)\n",
    "#             predicted = [asin  for asin,est in estimations[:k] if est >=threshold ]\n",
    "           \n",
    "    \n",
    "#         act_set = set(actual[user])\n",
    "#         pred_set = set(predicted)\n",
    "#         result1 = round(len(act_set & pred_set) / float(len(act_set)), 2) if len(act_set)!=0 else 0\n",
    "#         result2 = 1 if len(act_set & pred_set) >= 1 else 0\n",
    "#         recalls[user] = result1\n",
    "#         hit[user] = result2\n",
    "        \n",
    "#         print ( i , result1 , result2) #, act_set,pred_set\n",
    "#         i = i+1\n",
    "#     return recalls , hit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93311f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, model,k=10, threshold=0 ):\n",
    "    \n",
    "    recalls = dict()\n",
    "    hit = dict()\n",
    "    ndcg = dict()\n",
    "    actual = defaultdict(list)\n",
    "    #ideal_relevance1 = defaultdict(list)\n",
    "    ideal_relevance = defaultdict(list)\n",
    "    asins =[]\n",
    "    users=[]\n",
    "    #df = pd.DataFrame(predictions, columns = ['a','b','c' , 'd' ,'e'])\n",
    "    \n",
    "    for uid, asin, true_r, est, _ in predictions:\n",
    "        users.append(uid)\n",
    "        asins.append(asin)            \n",
    "        #if (true_r >= threshold):\n",
    "        actual[uid].append(asin)\n",
    "        #ideal_relevance1[uid].append((asin , true_r))\n",
    "        ideal_relevance[uid].append(true_r)\n",
    "            \n",
    "            \n",
    "    asins = list(set(asins))\n",
    "    users = set(users)\n",
    "    i=1\n",
    "    for user in users:\n",
    "        \n",
    "        estimations = []\n",
    "        predicted = []\n",
    "        before_pred  = []\n",
    "        arr=[]\n",
    "        relevance=[]\n",
    "        ideal_relevance5=[]\n",
    "        tmp = random.randint(0, len(asins)-1)\n",
    "     \n",
    "        for x in range(100):\n",
    "         \n",
    "            while tmp in arr:\n",
    "                tmp = random.randint(0, len(asins)-1)\n",
    "             \n",
    "          \n",
    "            arr.append(tmp)\n",
    "            before_pred.append(asins[tmp])\n",
    "            \n",
    "        for asin in actual[user]:\n",
    "            if asin not in before_pred:\n",
    "                before_pred.append(asin)\n",
    "                 \n",
    "                    \n",
    "        if model =='neumf':\n",
    "            user1 = [user for i in range(len(before_pred))]\n",
    "            e = model2.predict([np.array(user1),np.array(before_pred)],verbose=False).T[0]\n",
    "            sort = np.argsort(e)\n",
    "            estimations = np.array(before_pred)[sort]\n",
    "            predicted = [asin  for asin in estimations[:k]  if  float(e[sort][np.where(estimations == asin)[0][0]]) >= threshold ]\n",
    "            array = np.array(ideal_relevance[user])\n",
    "            relevance = [array[actual[user].index(asin)]   if asin in actual[user] else 0  for asin in predicted   ]\n",
    "            \n",
    "        \n",
    "        if model ==\"svd\":\n",
    "            for asin in before_pred:\n",
    "                e = svd.predict(user,asin).est\n",
    "                estimations.append((asin ,e ))\n",
    "                \n",
    "            estimations.sort(key=lambda x: x[1], reverse=True)\n",
    "            predicted = [asin  for asin,est in estimations[:k] if est >=threshold  ]\n",
    "            array = np.array(ideal_relevance[user])\n",
    "            relevance = [array[actual[user].index(asin)]   if asin in actual[user] else 0   for asin in predicted  ]\n",
    "           \n",
    "    \n",
    "        act_set = set(actual[user])\n",
    "        pred_set = set(predicted)\n",
    "        \n",
    "        \n",
    "        result1 = round(len(act_set & pred_set) / float(len(act_set)), 2) if len(act_set)!=0 else 0\n",
    "        result2 = 1 if len(act_set & pred_set) >= 1 else 0\n",
    "        \n",
    "        \n",
    "        ideal_relevance5 = sorted(ideal_relevance[user] , reverse=True )\n",
    "        \n",
    "        dcg = 0\n",
    "        idcg = 0\n",
    "\n",
    "        for k in range(1, k+1):\n",
    "            # calculate rel_k values\n",
    "            rel_k = relevance[k-1]\n",
    "            if k <= len(ideal_relevance5): \n",
    "                ideal_rel_k = ideal_relevance5[k-1]\n",
    "            else:\n",
    "                ideal_rel_k = 0\n",
    "            # calculate dcg and idcg\n",
    "            dcg += rel_k / log2(1 + k)\n",
    "            idcg += ideal_rel_k / log2(1 + k)\n",
    "            # calcualte ndcg\n",
    "            result3 = dcg / idcg\n",
    "            #print(f\"NDCG@{k} = {round(ndcg, 2)}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        recalls[user] = result1\n",
    "        hit[user] = result2\n",
    "        ndcg[user] = result3\n",
    "        \n",
    "        print ( i , result1 , result2 , result3) #, act_set,pred_set\n",
    "        i = i+1\n",
    "    return recalls , hit , ndcg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_recommendation(userId , top_n):\n",
    "    for asin in top_n[userId]:\n",
    "        img_show_asin(asin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9127f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(ratingPath)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c961f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = ratings.sort_values('Rating', ascending=True)\n",
    "#ratings = ratings[ : 24844171-20100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19cd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings[['userID', 'itemID', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = SVD(n_factors= 30 , n_epochs= 20 , lr_all = 0.005 , reg_all = 0.02 , random_state=42  )\n",
    "cross_validate(svd, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.20 , shuffle=True , random_state=42)\n",
    "\n",
    "#trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "#testset = trainset.build_testset()\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "accuracy.rmse(predictions)\n",
    "# Then compute RMSE\n",
    "#trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83db4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls , hits , ndcgs = precision_recall_at_k(predictions,'svd', k= 20)\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "print(sum(hit for hit in hits.values()) / len(hits))\n",
    "print(sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(len(predictions)) :\n",
    "  y.append(predictions[i].est)\n",
    "y = pd.DataFrame(y)\n",
    "y = y[0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af02d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(testset)\n",
    "x = df[2]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc81562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh (num , thresh = 3.5):\n",
    "    if num >  thresh:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= y.apply(thresh)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = x.apply(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ac101",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_test , pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8281a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.userId.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe38846",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd.predict('A2HM2RMX1CBZGM','B0006TIA8Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c07a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the products that has not been rated yet\n",
    "def build_anti_testset(self, userId, fill=None ):\n",
    "        \"\"\"Return a list of ratings that can be used as a testset in the\n",
    "        :meth:`test() <surprise.prediction_algorithms.algo_base.AlgoBase.test>`\n",
    "        method.\n",
    "\n",
    "        The ratings are all the ratings that are **not** in the trainset, i.e.\n",
    "        all the ratings :math:`r_{ui}` where the user :math:`u` is known, the\n",
    "        item :math:`i` is known, but the rating :math:`r_{ui}`  is not in the\n",
    "        trainset. As :math:`r_{ui}` is unknown, it is either replaced by the\n",
    "        :code:`fill` value or assumed to be equal to the mean of all ratings\n",
    "        :meth:`global_mean <surprise.Trainset.global_mean>`.\n",
    "\n",
    "        Args:\n",
    "            fill(float): The value to fill unknown ratings. If :code:`None` the\n",
    "                global mean of all ratings :meth:`global_mean\n",
    "                <surprise.Trainset.global_mean>` will be used.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples ``(uid, iid, fill)`` where ids are raw ids.\n",
    "        \"\"\"\n",
    "        fill = self.global_mean if fill is None else float(fill)\n",
    "\n",
    "        anti_testset = []\n",
    "        user_items = {j for (j, _) in self.ur[userId]}\n",
    "        anti_testset += [\n",
    "                (self.to_raw_uid(real_trainset.to_inner_uid(userId)), self.to_raw_iid(i), fill)\n",
    "                for i in self.all_items()\n",
    "                if i not in user_items\n",
    "            ]\n",
    "        return anti_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf09518",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_trainset = data.build_full_trainset()\n",
    "svd.fit(real_trainset)\n",
    "real_testset = build_anti_testset(real_trainset   , 'A2HM2RMX1CBZGM' )\n",
    "predictions = svd.test(real_testset)\n",
    "top_n = get_top_n(predictions, n=20)\n",
    "collaborative_recommendation('A2HM2RMX1CBZGM' , top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.expanduser(r\"C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\dump_file\")\n",
    "dump.dump(file_name, algo=svd)\n",
    "_, loaded_algo = dump.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c4204",
   "metadata": {},
   "source": [
    "# embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fc1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense , Concatenate , StringLookup, BatchNormalization,Dropout \n",
    "from keras.models import Model , load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f2ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dataset = pd.read_csv(ratingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21705947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AVU1ILDDYW301     190\n",
       "A2503LT8PZIHAD    147\n",
       "A2OCDK0BOW6UCY    108\n",
       "A1QBOC76MIOJYP     82\n",
       "A2RYWPOL4NN2KG     79\n",
       "                 ... \n",
       "A2D2CZMETCWS8S      6\n",
       "A2YTDLF0BFOCCA      6\n",
       "A2KWD1V1105XR7      6\n",
       "A30M4SJ2FCYDTK      6\n",
       "A345N1VP5CBPJU      6\n",
       "Name: userID, Length: 12711, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_dataset.userID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df763f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B00SU52460    1704\n",
       "B00VJOT0CK    1671\n",
       "B00X8AMIDG    1662\n",
       "B00SU5244M    1655\n",
       "B00WRGPHY4    1653\n",
       "              ... \n",
       "B0165ML58W       3\n",
       "B01E9MLVD4       3\n",
       "B00E0G7F4Y       3\n",
       "B000TGPR4O       3\n",
       "B00K8PQ6U0       3\n",
       "Name: itemID, Length: 14867, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_dataset.itemID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings_dataset.columns = ['userId' , 'asin' ,'rating' ,'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c791e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(ratings_dataset, test_size=0.2, random_state=42 , shuffle=True)\n",
    "\n",
    "#train = pd.read_csv(r'C:\\Users\\Abdo\\Desktop\\train.csv')\n",
    "#test = pd.read_csv(r'C:\\Users\\Abdo\\Desktop\\test.csv')\n",
    "\n",
    "\n",
    "n_users = len(ratings_dataset['userID'].unique())\n",
    "n_items = len(ratings_dataset['itemID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userId_lookup = StringLookup(vocabulary = ratings_dataset['userId'].unique(), mask_token = None )\n",
    "# asin_lookup = StringLookup(vocabulary = ratings_dataset['asin'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "# # creating item embedding\n",
    "# item_input = Input(shape=[1], name=\"Item-Input\" , dtype= object)\n",
    "# item_input1 = asin_lookup(item_input)\n",
    "# item_embedding = Embedding(n_items+1, 64, name=\"Item-Embedding\")(item_input1) # fourth root of unique values\n",
    "# item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "\n",
    "# # creating user embedding\n",
    "# user_input = Input(shape=[1], name=\"User-Input\" , dtype= object)\n",
    "# user_input1 = userId_lookup(user_input)\n",
    "# user_embedding = Embedding(n_users+1, 64, name=\"User-Embedding\")(user_input1)\n",
    "# user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "\n",
    "# prod = Dot(name=\"Dot-Product\", axes=1)([item_vec, user_vec])\n",
    "\n",
    "# # concatenate features\n",
    "# conc = Concatenate()([item_vec, user_vec])\n",
    "\n",
    "# # add fully-connected-layers\n",
    "# fc1 = Dense(512, activation='relu')(conc)\n",
    "\n",
    "# fc1_bn = BatchNormalization(name='batch-norm-1')(fc1)\n",
    "# fc1_dropout = Dropout(0.4)(fc1_bn)\n",
    "\n",
    "\n",
    "# fc2 = Dense(256, activation='relu')(fc1_dropout)\n",
    "\n",
    "# fc2_bn = BatchNormalization(name='batch-norm-2')(fc2)\n",
    "# fc2_dropout = Dropout(0.4)(fc2_bn)\n",
    "\n",
    "# fc3 = Dense(128, activation='relu')(fc2_dropout)\n",
    "\n",
    "# fc3_bn = BatchNormalization(name='batch-norm-3')(fc3)\n",
    "# fc3_dropout = Dropout(0.4)(fc3_bn)\n",
    "\n",
    "\n",
    "# pred_mlp = Dense(64, name='pred-mlp', activation='relu')(fc3_dropout)\n",
    "\n",
    "# fc3 = Concatenate()([prod, pred_mlp])\n",
    "\n",
    "# out = Dense(1 , activation='relu' )(fc3)\n",
    "\n",
    "# # Create model and compile it\n",
    "# model2 = Model([user_input, item_input], out)\n",
    "# model2.compile(optimizer=keras.optimizers.Adam(lr=0.005), loss= tf.keras.losses.mean_absolute_error  , metrics= tf.keras.metrics.RootMeanSquaredError(\n",
    "#     name=\"root_mean_squared_error\", dtype=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade2f53c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'warmstart_embedding_matrix' from 'tensorflow.keras.utils' (C:\\Users\\Abdo\\anaconda3\\lib\\site-packages\\keras\\api\\_v2\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warmstart_embedding_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'warmstart_embedding_matrix' from 'tensorflow.keras.utils' (C:\\Users\\Abdo\\anaconda3\\lib\\site-packages\\keras\\api\\_v2\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import warmstart_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f895d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embedding = tf.keras.layers.Embedding(3, 3)\n",
    "new_embedding.build(input_shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbdb41a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e99ad085",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3697869771.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [24]\u001b[1;36m\u001b[0m\n\u001b[1;33m    new_embeddings_initializer=\"uniform\")\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "new_embedding.embeddings.assign(tf.keras.utils.warmstart_embedding_matrix(base_vocabulary=base_vectorization.get_vocabulary(),\n",
    "        new_vocabulary=new_vectorization.get_vocabulary(),\n",
    "        base_embeddings=base_embedding.embeddings,\n",
    "        new_embeddings_initializer=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "userId_lookup = StringLookup(vocabulary = ratings_dataset['userID'].unique(), mask_token = None )\n",
    "asin_lookup = StringLookup(vocabulary = ratings_dataset['itemID'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "item_input = Input(shape=[1],name='item-input'  , dtype= object)\n",
    "user_input = Input(shape=[1], name='user-input' , dtype= object)\n",
    "\n",
    "# MLP Embeddings\n",
    "item_input1 = asin_lookup(item_input)\n",
    "user_input1 = userId_lookup(user_input)\n",
    "\n",
    "item_embedding_mlp = Embedding(n_items + 1, 16, name='item-embedding-mlp'  )(item_input1)\n",
    "item_vec_mlp = Flatten(name='flatten-item-mlp')(item_embedding_mlp)\n",
    "\n",
    "user_embedding_mlp = Embedding(n_users + 1, 16, name='user-embedding-mlp' )(user_input1)\n",
    "user_vec_mlp = Flatten(name='flatten-user-mlp')(user_embedding_mlp)\n",
    "\n",
    "# MF Embeddings\n",
    "item_embedding_mf = Embedding(n_items + 1, 16, name='item-embedding-mf' , embeddings_regularizer='L2' )(item_input1)\n",
    "item_vec_mf = Flatten(name='flatten-item-mf')(item_embedding_mf)\n",
    "\n",
    "user_embedding_mf = Embedding(n_users + 1, 16, name='user-embedding-mf' , embeddings_regularizer='L2' )(user_input1)\n",
    "user_vec_mf = Flatten(name='flatten-user-mf')(user_embedding_mf)\n",
    "\n",
    "# MLP layers\n",
    "\n",
    "concat = Concatenate()([item_vec_mlp, user_vec_mlp])\n",
    "concat_dropout = Dropout(0.5)(concat)\n",
    "\n",
    "fc_1 = Dense(128, name='fc-1', activation='relu')(concat_dropout)\n",
    "fc_1_bn = BatchNormalization(name='batch-norm-1')(fc_1)\n",
    "fc_1_dropout = Dropout(0.5)(fc_1_bn)\n",
    "\n",
    "fc_2 = Dense(64, name='fc-2', activation='relu')(fc_1_dropout)\n",
    "fc_2_bn = BatchNormalization(name='batch-norm-2')(fc_2)\n",
    "fc_2_dropout = Dropout(0.5)(fc_2_bn)\n",
    "\n",
    "\n",
    "# fc_3 = Dense(32, name='fc-3', activation='relu')(fc_2_dropout)\n",
    "# fc_3_bn = BatchNormalization(name='batch-norm-3')(fc_3)\n",
    "# fc_3_dropout = Dropout(0.5)(fc_3_bn)\n",
    "\n",
    "\n",
    "# Prediction from both layers\n",
    "pred_mlp = Dense(16, name='pred-mlp', activation='relu' )(fc_2_dropout)\n",
    "\n",
    "\n",
    "\n",
    "pred_mf = Dot(name=\"Dot-Product\", axes=1)([item_vec_mf, user_vec_mf])\n",
    "combine_mlp_mf = Concatenate()([pred_mf, pred_mlp])\n",
    "\n",
    "# Final prediction\n",
    "result = Dense(1, name='result', activation='relu' )(combine_mlp_mf)\n",
    "\n",
    "model2 = Model([user_input, item_input], result)\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss= tf.keras.losses.mean_absolute_error  , metrics= tf.keras.metrics.RootMeanSquaredError(\n",
    "    name=\"root_mean_squared_error\", dtype=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ec54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import warmstart_embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_vocabulary_user = ratings_dataset['userID'].unique()\n",
    "base_vocabulary_item = ratings_dataset['itemID'].unique()\n",
    "# old_text_vectorization_layer.get_vocabulary()\n",
    "new = pd.read_csv(r'C:\\Users\\Sigma\\ecommerce\\new.csv')\n",
    "new_vocabulary_user = new['userID'].unique()\n",
    "new_vocabulary_item = new['itemID'].unique()\n",
    "# new_text_vectorization_layer.get_vocabulary()\n",
    "# get previous embedding layer weights\n",
    "\n",
    "\n",
    "embedding_weights_base_item1 = model2.get_layer('item-embedding-mlp').get_weights()[0]\n",
    "embedding_weights_base_item2 = model2.get_layer('item-embedding-mf').get_weights()[0]\n",
    "embedding_weights_base_user1 = model2.get_layer('user-embedding-mlp').get_weights()[0]\n",
    "embedding_weights_base_user2 = model2.get_layer('user-embedding-mf').get_weights()[0]\n",
    "\n",
    "\n",
    "\n",
    "warmstarted_embedding1 = warmstart_embedding_matrix(\n",
    "                              base_vocabulary_user,\n",
    "                              new_vocabulary_user,\n",
    "                              base_embeddings=embedding_weights_base_user1,\n",
    "                              new_embeddings_initializer=\"uniform\")\n",
    "updated_embedding_variable1 = tf.Variable(warmstarted_embedding1)\n",
    "\n",
    "# update embedding layer weights ####################################33\n",
    "model2.layers[5].embeddings = updated_embedding_variable1\n",
    "\n",
    "\n",
    "warmstarted_embedding2 = warmstart_embedding_matrix(\n",
    "                              base_vocabulary_user,\n",
    "                              new_vocabulary_user,\n",
    "                              base_embeddings=embedding_weights_base_user2,\n",
    "                              new_embeddings_initializer=\"uniform\")\n",
    "updated_embedding_variable2 = tf.Variable(warmstarted_embedding2)\n",
    "\n",
    "# update embedding layer weights  ####################################\n",
    "model2.layers[15].embeddings = updated_embedding_variable2\n",
    "\n",
    "\n",
    "warmstarted_embedding3 = warmstart_embedding_matrix(\n",
    "                              base_vocabulary_item,\n",
    "                              new_vocabulary_item,\n",
    "                              base_embeddings=embedding_weights_base_item1,\n",
    "                              new_embeddings_initializer=\"uniform\")\n",
    "updated_embedding_variable3 = tf.Variable(warmstarted_embedding3)\n",
    "\n",
    "# update embedding layer weights  ####################################\n",
    "model2.layers[4].embeddings = updated_embedding_variable3\n",
    "\n",
    "\n",
    "warmstarted_embedding4 = warmstart_embedding_matrix(\n",
    "                              base_vocabulary_item,\n",
    "                              new_vocabulary_item,\n",
    "                              base_embeddings=embedding_weights_base_item2,\n",
    "                              new_embeddings_initializer=\"uniform\")\n",
    "updated_embedding_variable4 = tf.Variable(warmstarted_embedding4)\n",
    "\n",
    "# update embedding layer weights\n",
    "model2.layers[14].embeddings = updated_embedding_variable4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53068f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new5 = pd.read_csv(r'C:\\Users\\Sigma\\ecommerce\\new1.csv')\n",
    "history = model2.fit([new5.userID, new5.itemID], new5['rating'] , batch_size=64  , epochs=35,  verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "new['userID'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a439b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = test['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5'):\n",
    "    model2 = load_model(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "else:\n",
    "    #history = model2.fit([train.userId, train.asin], labels, batch_size=64  , epochs=10 ,  verbose=1)\n",
    "    history = model2.fit([ratings_dataset.userID, ratings_dataset.itemID], ratings_dataset['rating'] , batch_size=64  , epochs= 10 ,  verbose=1 )\n",
    "    model2.save(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Training Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.get_layer('user-embedding-mf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0534230",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model2.layers[15].get_weights()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "new['userID'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #userId_lookup = StringLookup(vocabulary = ratings_dataset['userId'].unique(), mask_token = None )\n",
    "# #asin_lookup = StringLookup(vocabulary = ratings_dataset['asin'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "# item_input = Input(shape=[1],name='item-input' )\n",
    "# user_input = Input(shape=[1], name='user-input' )\n",
    "\n",
    "# # MLP Embeddings\n",
    "# #item_input1 = asin_lookup(item_input)\n",
    "# #user_input1 = userId_lookup(user_input)\n",
    "# item_input1 = item_input\n",
    "# user_input1 = user_input\n",
    "\n",
    "# item_embedding_mlp = Embedding(n_items + 1, 16, name='item-embedding-mlp' )(item_input1)\n",
    "# item_vec_mlp = Flatten(name='flatten-item-mlp')(item_embedding_mlp)\n",
    "\n",
    "# user_embedding_mlp = Embedding(n_users + 1, 16, name='user-embedding-mlp'  )(user_input1)\n",
    "# user_vec_mlp = Flatten(name='flatten-user-mlp')(user_embedding_mlp)\n",
    "\n",
    "# # MF Embeddings\n",
    "# item_embedding_mf = Embedding(n_items + 1, 4, name='item-embedding-mf' )(item_input1)\n",
    "# item_vec_mf = Flatten(name='flatten-item-mf')(item_embedding_mf)\n",
    "\n",
    "# user_embedding_mf = Embedding(n_users + 1, 4, name='user-embedding-mf' )(user_input1)\n",
    "# user_vec_mf = Flatten(name='flatten-user-mf')(user_embedding_mf)\n",
    "\n",
    "# # MLP layers\n",
    "\n",
    "# concat = Concatenate()([item_vec_mlp, user_vec_mlp])\n",
    "# concat_dropout = Dropout(0.3)(concat)\n",
    "\n",
    "# fc_1 = Dense(16, name='fc-1', activation='relu' )(concat_dropout)\n",
    "# fc_1_bn = BatchNormalization(name='batch-norm-1')(fc_1)\n",
    "# fc_1_dropout = Dropout(0.3)(fc_1_bn)\n",
    "\n",
    "# # fc_2 = Dense(8, name='fc-2', activation='relu', kernel_regularizer='L2')(fc_1_dropout)\n",
    "# # fc_2_bn = BatchNormalization(name='batch-norm-2')(fc_2)\n",
    "# # fc_2_dropout = Dropout(0.3)(fc_2_bn)\n",
    "\n",
    "\n",
    "# # fc_3 = Dense(64, name='fc-3', activation='relu')(fc_2_dropout)\n",
    "# # fc_3_bn = BatchNormalization(name='batch-norm-3')(fc_3)\n",
    "# # fc_3_dropout = Dropout(0.4)(fc_3_bn)\n",
    "\n",
    "\n",
    "# # Prediction from both layers\n",
    "# pred_mlp = Dense(8, name='pred-mlp', activation='relu' )(fc_1_dropout)\n",
    "\n",
    "\n",
    "\n",
    "# pred_mf = Dot(name=\"Dot-Product\", axes=1)([item_vec_mf, user_vec_mf])\n",
    "# combine_mlp_mf = Concatenate()([pred_mf, pred_mlp])\n",
    "\n",
    "# # Final prediction\n",
    "# result = Dense(1, name='result', activation='sigmoid')(combine_mlp_mf)\n",
    "\n",
    "# model2 = Model([user_input, item_input], result)\n",
    "# model2.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522dcd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# userId_lookup = StringLookup(vocabulary = ratings_dataset['userId'].unique(), mask_token = None )\n",
    "# asin_lookup = StringLookup(vocabulary = ratings_dataset['asin'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "# # creating item embedding\n",
    "# item_input = Input(shape=[1], name=\"Item-Input\" , dtype= object)\n",
    "# item_input1 = asin_lookup(item_input)\n",
    "# item_embedding = Embedding(n_items+1, 8, name=\"Item-Embedding\")(item_input1) # fourth root of unique values\n",
    "# item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "\n",
    "# # creating user embedding\n",
    "# user_input = Input(shape=[1], name=\"User-Input\" , dtype= object)\n",
    "# user_input1 = userId_lookup(user_input)\n",
    "# user_embedding = Embedding(n_users+1, 8, name=\"User-Embedding\")(user_input1)\n",
    "# user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "\n",
    "# prod = Dot(name=\"Dot-Product\", axes=1)([item_vec, user_vec])\n",
    "\n",
    "# # concatenate features\n",
    "# conc = Concatenate()([item_vec, user_vec])\n",
    "\n",
    "# # add fully-connected-layers\n",
    "# fc1 = Dense(16, activation='relu')(conc)\n",
    "\n",
    "# fc1_bn = BatchNormalization(name='batch-norm-1')(fc1)\n",
    "# fc1_dropout = Dropout(0.3)(fc1_bn)\n",
    "\n",
    "# # fc2 = Dense(32, activation='relu')(fc1_dropout)\n",
    "\n",
    "# # fc2_bn = BatchNormalization(name='batch-norm-2')(fc2)\n",
    "# # fc2_dropout = Dropout(0.3)(fc2_bn)\n",
    "\n",
    "# pred_mlp = Dense(8, name='pred-mlp', activation='relu')(fc1_dropout)\n",
    "\n",
    "# fc3 = Concatenate()([prod, pred_mlp])\n",
    "\n",
    "# out = Dense(1 , activation='sigmoid' )(fc3)\n",
    "\n",
    "# # Create model and compile it\n",
    "# model2 = Model([user_input, item_input], out)\n",
    "# model2.compile(optimizer='adam', loss='binary_crossentropy'  , metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels1 = test['rating'].apply(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fadcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = test['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d128a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate([test.userID, test.itemID], labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels1 = test['rating'].apply(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2841a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict([test['userID'], test['itemID']])\n",
    "\n",
    "# i = 0\n",
    "# for p in predictions:\n",
    "#     predictions[i] = thresh(p,3.5)\n",
    "#     i = i+1\n",
    "\n",
    "[print(predictions[i], test.rating.iloc[i]) for i in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e889e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2['new_column'] = predictions.T[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c321418",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747875",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2['timestamp'] = predictions.T[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ee3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test2[['userID','itemID','rating', 'new_column' ,'timestamp' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ba45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = test2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfa21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls , hits , ndcgs = precision_recall_at_k(test3,'neumf', k= 20)\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "print(sum(hit for hit in hits.values()) / len(hits))\n",
    "print(sum(ndcg for ndcg in ndcgs.values()) / len(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04023c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = np.array(list(set(ratings_dataset.asin)))\n",
    "item_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for user 'AZB4CQ9JZSUQB'\n",
    "user = np.array(['AZB4CQ9JZSUQB' for i in range(len(item_data))])\n",
    "user[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict([user, item_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbe717",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920bd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict([user, item_data])\n",
    "print(predictions)\n",
    "\n",
    "predictions = np.array([a[0] for a in predictions])\n",
    "\n",
    "recommended_indices = (-predictions).argsort()[:5]\n",
    "\n",
    "# print the best five items \n",
    "print(item_data[recommended_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[recommended_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['asin'].isin(item_data[recommended_indices])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = test['rating'].apply(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa05d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels1, np.round(predictions) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(labels1 , np.round(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732a1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3440d6d0",
   "metadata": {},
   "source": [
    "# train test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f48a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df3.userId.unique()) + len(df3.asin.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = df3.drop_duplicates(subset=['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d621d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = df3.drop_duplicates(subset=['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ac261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = pd.concat([train1,train2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = train3.drop_duplicates(subset = ['userId', 'asin']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51335133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted = pd.concat([train3, df3])\n",
    "subtracted = subtracted.drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86290bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted = subtracted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " subtracted2= subtracted[0:17000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed83c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalisa = pd.concat([train3,subtracted2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = finalisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b413788",
   "metadata": {},
   "outputs": [],
   "source": [
    " test = subtracted[17000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373429d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(r'C:\\Users\\Abdo\\Desktop\\train.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2657317",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r'C:\\Users\\Abdo\\Desktop\\test.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['userId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4a4e7",
   "metadata": {},
   "source": [
    "# recommendation for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os # accessing directory structure\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "import cv2\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import requests\n",
    "import PIL.Image as Image\n",
    "import io\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887afb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Shape\n",
    "img_width, img_height, _ = 224, 224, 3 #load_image(df.iloc[0].image).shape\n",
    "\n",
    "# Pre-Trained Model\n",
    "base_model = ResNet50(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape = (img_width, img_height, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add Layer Embedding\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalMaxPooling2D()\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc624f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this code is to store the images from the url\n",
    "\n",
    "# for asin in df2['asin']:\n",
    "#     image_url = literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0]\n",
    "#     img_data = requests.get(image_url).content\n",
    "#     #img_data = img_data.decode('utf-8')\n",
    "#     #img_data = img_data.encode()\n",
    "#     #arr = np.asarray(bytearray(img_data), dtype=np.uint8)\n",
    "#     #img_data = cv2.imdecode(arr, -1 )\n",
    "#     #img_data = Image.open(io.BytesIO(img_data))\n",
    "#     #height, width, channels = scipy.ndimage.imread(img_data).shape\n",
    "    \n",
    "    \n",
    "    \n",
    "#     img_data = np.asarray(bytearray(img_data), dtype=np.uint8)\n",
    "#     img_data = cv2.imdecode(img_data, -1 ) # 'Load it as it is'\n",
    "#     #img_data = Image.open(BytesIO(img_data))\n",
    "#     #print(type(img_data))\n",
    "#     #if  len(img_data.shape)== 2:\n",
    "#         #img_data = np.expand_dims(img_data, axis=1)\n",
    "#     if  len(img_data.shape) > 2:\n",
    "#         if img_data.shape[2] ==1:\n",
    "#             img_data = np.squeeze(img_data, axis=2)  # axis=2 is channel dimension \n",
    "        \n",
    "        \n",
    "#     img_data = Image.fromarray(img_data)\n",
    "      \n",
    "#     # saving the final output \n",
    "#     # as a PNG file\n",
    "#     img_data.save('C:\\\\Users\\\\Abdo\\\\Desktop\\\\last year\\\\project\\\\datasets\\\\last datataset isa\\\\images' + \"\\\\\" + asin + '.jpg')\n",
    "        \n",
    "#    # with open('C:\\\\Users\\\\Abdo\\\\Desktop\\\\last year\\\\project\\\\datasets\\\\last datataset isa\\\\images' + \"\\\\\" + asin + '.jpg', 'wb') as handler:\n",
    "#         #handler.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbf9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding1(model, asin):\n",
    "    # Reshape\n",
    "    img = cv2.imread('C:\\\\Users\\\\Sigma\\\\ecommerce\\\\images' + \"\\\\\" + asin + '.jpg')\n",
    "        \n",
    "    if img.shape[2] == 1:\n",
    "        img = cv2.merge((img,img,img))   \n",
    "    \n",
    "    \n",
    "    #img = tf.keras.utils.load_img(img, target_size=(img_width, img_height))\n",
    "    \n",
    "    width = 224\n",
    "    height = 224\n",
    "    \n",
    "    dsize = (width, height)\n",
    "    output = cv2.resize(img, dsize)\n",
    "    \n",
    "    # img to Array\n",
    "    x   = img_to_array(output)\n",
    "    # Expand Dim (1, w, h)\n",
    "    x   = np.expand_dims(x, axis=0)\n",
    "    # Pre process Input\n",
    "    x   = preprocess_input(x)\n",
    "    return model.predict(x , verbose = False).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d324d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_embedding2(model, asin):\n",
    "#     # Reshape\n",
    "    \n",
    "#     #response = requests.get(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0])\n",
    "#     #img = Image.open(BytesIO(response.content))\n",
    "#     print (asin)\n",
    "    \n",
    "#     with urllib.request.urlopen(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0]) as url:\n",
    "#         s = url.read()\n",
    "    \n",
    "#     #req = urllib.urlopen(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0])\n",
    "#     arr = np.asarray(bytearray(s), dtype=np.uint8)\n",
    "#     img = cv2.imdecode(arr, -1 ) # 'Load it as it is'\n",
    "    \n",
    "#     if len(img.shape) == 2:\n",
    "#         img = img[:,: ,np.newaxis]\n",
    "    \n",
    "        \n",
    "#     if img.shape[2] == 1:\n",
    "#         img = cv2.merge((img,img,img))   \n",
    "    \n",
    "    \n",
    "#     #img = tf.keras.utils.load_img(img, target_size=(img_width, img_height))\n",
    "    \n",
    "#     width = 224\n",
    "#     height = 224\n",
    "    \n",
    "#     dsize = (width, height)\n",
    "#     output = cv2.resize(img, dsize)\n",
    "    \n",
    "#     # img to Array\n",
    "#     x   = img_to_array(output)\n",
    "#     # Expand Dim (1, w, h)\n",
    "#     x   = np.expand_dims(x, axis=0)\n",
    "#     # Pre process Input\n",
    "#     x   = preprocess_input(x)\n",
    "#     return model.predict(x , verbose = False).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = get_embedding1(model, \"B00KO4SBR6\")\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483261ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show_asin(\"B00EZ6L6B2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881337d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sample      = df2\n",
    "map_embeddings = df_sample['asin'].apply(lambda img: get_embedding1(model, img))\n",
    "df_embs        = map_embeddings.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b976b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1240f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim3 = cosine_similarity(df_embs,df_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ec335",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices3 = pd.Series(df2.index, index=df2['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce51fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations( 'B00EZ6L6B2', cosine_sim3 , indices3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c2662",
   "metadata": {},
   "source": [
    "# handling ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(r'C:\\Users\\Abdo\\Desktop\\mini_metadata32000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fbafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\Books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e607b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.columns = ['userId' ,'asin',  'rating' , 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fd071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.columns = ['asin', 'userId' , 'rating' , 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0d3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[['userId','asin' , 'rating' , 'timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce653",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c07f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ratings['userId'].value_counts()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maping = pd.DataFrame(ratings['userId'].value_counts())\n",
    "condition = maping[(maping['userId'] >= 35) &(maping['userId'] <= 1000)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ffa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[ratings['userId'].isin(condition)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86161db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maping2 = pd.DataFrame(ratings['asin'].value_counts())\n",
    "condition2 = maping2[(maping2['asin'] >= 35)&  (maping2['asin'] <= 1000) ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[ratings['asin'].isin(condition2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b957cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ratings['asin'].unique()\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['userId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['asin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc33bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.drop('asin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d21726",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['asin'] = t[0:31858]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[ratings['asin'].isin(meta['asin'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef598a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.to_csv(r'C:\\Users\\Abdo\\Desktop\\last_mini_metadata32000.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.rename(columns = {'movieId':'asin'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[ratings['rating'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af238ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b32e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings = ratings[:19536645-2800000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings['asin'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94473c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['asin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "y = ratings['userId'].value_counts()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e708d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(31858):\n",
    "    if (y.iloc[i] == 1 ):\n",
    "        x = x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf846d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.to_csv(r'C:\\Users\\Abdo\\Desktop\\moviesonly32000.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[:22163126-2000000]['asin'].value_counts().tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[:22163126-2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb70e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
