{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb19428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from ast import literal_eval\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense , Concatenate , StringLookup, BatchNormalization,Dropout\n",
    "from keras.models import Model , load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from surprise import Reader, Dataset, SVD , dump\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import os # accessing directory structure\n",
    "import keras\n",
    "from keras import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "import cv2\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import requests\n",
    "import PIL.Image as Image\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5b18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\notFound2_metadata_votecount5407.csv'\n",
    "ratingPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\uniqueratings.csv'\n",
    "df2= pd.read_csv(dataPath)\n",
    "df3= pd.read_csv(ratingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b1e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_indices( product_indices):\n",
    "    fig, axs = plt.subplots(1, 10,figsize=(18,4))\n",
    "    fig.suptitle('You may also like these products', size = 22)\n",
    "    i = 0\n",
    "    for ind in product_indices:\n",
    "        response = requests.get(literal_eval(df2['imageURLHighRes'][ind])[0])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].set_title('{}  '.format(round(df2['vote_average'][ind],2)),y=-0.18,color=\"red\",fontsize=18)\n",
    "        i = i+1\n",
    "        fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8699f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_asin(asin):\n",
    "    response = requests.get(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "    plt.title(label= '{}  '.format(round(df2[df2['asin'] == asin]['vote_average'].values[0],2)) ,\n",
    "          loc=\"right\",\n",
    "          color='red' , fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45082687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(asin, cosine_sim , indices , numItems):\n",
    "    \n",
    "#     img_show_asin(asin)\n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # list of tuples [(0,1) , (1,0.5)]\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:numItems+1]  # don't take 0 as we don't take the score with the same product\n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    arr =  np.array(df2['asin'].iloc[product_indices])\n",
    "    \n",
    "    return (arr)\n",
    "#     img_show_indices( product_indices)\n",
    "#     print(arr)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b821d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding1(model, asin):\n",
    "    # Reshape \n",
    "    img = cv2.imread('C:\\\\Users\\\\Abdo\\\\Desktop\\\\last year\\\\project\\\\datasets\\\\last datataset isa\\\\images' + \"\\\\\" + asin + '.jpg')\n",
    "        \n",
    "    if img.shape[2] == 1:\n",
    "        img = cv2.merge((img,img,img))   \n",
    "    \n",
    "    \n",
    "    #img = tf.keras.utils.load_img(img, target_size=(img_width, img_height))\n",
    "    \n",
    "    width = 224\n",
    "    height = 224\n",
    "    \n",
    "    dsize = (width, height)\n",
    "    output = cv2.resize(img, dsize)\n",
    "    \n",
    "    # img to Array\n",
    "    x   = img_to_array(output)\n",
    "    # Expand Dim (1, w, h)\n",
    "    x   = np.expand_dims(x, axis=0)\n",
    "    # Pre process Input\n",
    "    x   = preprocess_input(x)\n",
    "    return model.predict(x , verbose = False).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da355bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_recommendation(userId , top_n):\n",
    "    \n",
    "    a = df3[df3['userId'] == userId]\n",
    "    \n",
    "    asinArr = []\n",
    "    for asin in top_n[userId]:\n",
    "        a = a[a['asin'] == asin]\n",
    "        if (a.empty):\n",
    "            asinArr.append(asin[0])\n",
    "    return asinArr\n",
    "        \n",
    "#         img_show_asin(asin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b228dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the input is new to the dataset and not yet calculate its cosine with other products\n",
    "def tf_idftransformer (asin) :\n",
    "    x = tfidf.transform(  df2[df2['asin']== asin]['description'] + \" \" +df2[df2['asin']== asin]['feature'] )\n",
    "    cosine_sim = cosine_similarity(x, tfidf_matrix)\n",
    "    #cosine_sim1 = cosine_sim1 + cosine_sim  \n",
    "    indices = pd.Series(df2.index, index=df2['asin'])\n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0cc85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transformer (asin) :\n",
    "    x = count.transform(  df2[df2['asin']== asin]['brand'] + \" \" +df2[df2['asin']== asin]['category'] )\n",
    "    cosine_sim = cosine_similarity(x, count_matrix)\n",
    "    indices = pd.Series(df2.index, index=df2['asin'])\n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39fdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363e40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the products that has not been rated yet\n",
    "def build_anti_testset(self, userId, fill=None ):\n",
    "        \"\"\"Return a list of ratings that can be used as a testset in the\n",
    "        :meth:`test() <surprise.prediction_algorithms.algo_base.AlgoBase.test>`\n",
    "        method.\n",
    "\n",
    "        The ratings are all the ratings that are **not** in the trainset, i.e.\n",
    "        all the ratings :math:`r_{ui}` where the user :math:`u` is known, the\n",
    "        item :math:`i` is known, but the rating :math:`r_{ui}`  is not in the\n",
    "        trainset. As :math:`r_{ui}` is unknown, it is either replaced by the\n",
    "        :code:`fill` value or assumed to be equal to the mean of all ratings\n",
    "        :meth:`global_mean <surprise.Trainset.global_mean>`.\n",
    "\n",
    "        Args:\n",
    "            fill(float): The value to fill unknown ratings. If :code:`None` the\n",
    "                global mean of all ratings :meth:`global_mean\n",
    "                <surprise.Trainset.global_mean>` will be used.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples ``(uid, iid, fill)`` where ids are raw ids.\n",
    "        \"\"\"\n",
    "        fill = self.global_mean if fill is None else float(fill)\n",
    "\n",
    "        anti_testset = []\n",
    "        user_items = {j for (j, _) in self.ur[userId]}\n",
    "        anti_testset += [\n",
    "                (self.to_raw_uid(self.to_inner_uid(userId)), self.to_raw_iid(i), fill)\n",
    "                for i in self.all_items()\n",
    "                if i not in user_items\n",
    "            ]\n",
    "        return anti_testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8a1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh (num , thresh = 2.5):\n",
    "    if num >  thresh:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88f719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C= df2['vote_average'].mean()\n",
    "m= df2['vote_count'].quantile(0.9)\n",
    "df22 = df2.copy().loc[df2['vote_count'] >= m]\n",
    "\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5d1a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(dataPath)\n",
    "df3= pd.read_csv(ratingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d37d4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topRecom(numPerCategory = 5 ) :\n",
    "    \n",
    "    dataFramePhone = df22[df22['category'].str[2:27] == 'Cell Phones & Accessories']\n",
    "\n",
    "    dataFrameAppliances = df22[df22['category'].str[2:12] == 'Appliances']\n",
    "    dataFrameSports = df22[df22['category'].str[2:19] == 'Sports & Outdoors']\n",
    "    dataFramePhone['score'] = dataFramePhone.apply(weighted_rating, axis=1)\n",
    "\n",
    "    dataFrameAppliances['score'] = dataFrameAppliances.apply(weighted_rating, axis=1)\n",
    "    dataFrameSports['score'] = dataFrameSports.apply(weighted_rating, axis=1)\n",
    "    \n",
    "    dataFramePhone = dataFramePhone.sort_values('score', ascending=False)\n",
    "    dataFrameAppliances = dataFrameAppliances.sort_values('score', ascending=False)\n",
    "    dataFrameSports = dataFrameSports.sort_values('score', ascending=False)\n",
    "    \n",
    "    arr1 = np.array(dataFramePhone['asin'][0:round(numPerCategory/3)])\n",
    "    arr2 = np.array(dataFrameAppliances['asin'][0:round(numPerCategory/3)])\n",
    "    arr3 = np.array(dataFrameSports['asin'][0:round(numPerCategory/3)])\n",
    "    \n",
    "    coldstart = np.concatenate((arr1 , arr2 , arr3))\n",
    "\n",
    "    return coldstart\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5a4e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedDescription(asin , numItems = 5 ):\n",
    "    df2= pd.read_csv(dataPath)\n",
    "    if os.path.exists(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim1.pkl'):\n",
    "        with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim1.pkl','rb') as f:\n",
    "            cosine_sim1 = pickle.load(f)\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english') # playing--->play\n",
    "        analyzer = TfidfVectorizer().build_analyzer()\n",
    "        def stemmed_words(doc):\n",
    "            return (stemmer.stem(w) for w in analyzer(doc))\n",
    "        tfidf = TfidfVectorizer( analyzer = stemmed_words ,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 min_df=3/31858, # if the word is repeated less than three times then I dont care about it\n",
    "                                 max_df=0.5, # if the word is repeated more than 50% then I don't care about it\n",
    "                                 stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(  df2['description'] + \" \" + df2['feature'] )\n",
    "        cosine_sim1 = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim1.pkl','wb') as f:\n",
    "            pickle.dump(cosine_sim1, f)\n",
    "    indices1 = pd.Series(df2.index, index=df2['asin'])\n",
    "    \n",
    "    return (get_recommendations(asin , cosine_sim1  ,indices1 , numItems))\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad4aa0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedBrand(asin, numItems = 5   ):\n",
    "    df2= pd.read_csv(dataPath)\n",
    "    if os.path.exists(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim2.pkl'):\n",
    "        with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim2.pkl','rb') as f:\n",
    "            cosine_sim2 = pickle.load(f)\n",
    "    else:\n",
    "        count = CountVectorizer(stop_words='english')\n",
    "        count_matrix = count.fit_transform(df2['brand']+\" \"+ df2['category'])\n",
    "        cosine_sim2 = cosine_similarity(count_matrix,count_matrix)\n",
    "        with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim2.pkl','wb') as f:\n",
    "            pickle.dump(cosine_sim2, f)\n",
    "    indices2 = pd.Series(df2.index, index=df2['asin'])\n",
    "    \n",
    "    return (get_recommendations( asin, cosine_sim2 , indices2,numItems ))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31802ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collabrative (user , numItems = 5   ):\n",
    "    from surprise.model_selection import train_test_split\n",
    "    \n",
    "    ratings = pd.read_csv(ratingPath)\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(ratings[['userId', 'asin', 'rating']], reader)\n",
    "    real_trainset = data.build_full_trainset()\n",
    "    if os.path.exists(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\dump_file'):\n",
    "        file_name = os.path.expanduser(r\"C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\dump_file\")\n",
    "        _,svd = dump.load(file_name)\n",
    "       \n",
    "    else:\n",
    "    \n",
    "        \n",
    "        svd = SVD(n_factors= 30 , n_epochs= 20 , lr_all = 0.005 , reg_all = 0.02 , random_state=42  )\n",
    "        svd.fit(real_trainset)\n",
    "        file_name = os.path.expanduser(r\"C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\dump_file\")\n",
    "        dump.dump(file_name, algo=svd)\n",
    "\n",
    "    \n",
    "   \n",
    "    real_trainset = data.build_full_trainset()\n",
    "    real_testset = build_anti_testset(real_trainset   , user )\n",
    "    predictions = svd.test(real_testset)\n",
    "    top_n = get_top_n(predictions, n=numItems)\n",
    "    \n",
    "    \n",
    "    return (collaborative_recommendation(user , top_n))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d414d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(userId , numItems = 5 ):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    ratings_dataset = pd.read_csv(ratingPath)\n",
    "    if os.path.exists(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\embedding_model.h5'):\n",
    "        model2 = load_model(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\embedding_model.h5')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        train, test = train_test_split(ratings_dataset, test_size=0.2, random_state=42 , shuffle=True)\n",
    "        n_users = len(ratings_dataset['userId'].unique())\n",
    "        n_items = len(ratings_dataset['asin'].unique())\n",
    "        userId_lookup = StringLookup(vocabulary = ratings_dataset['userId'].unique(), mask_token = None )\n",
    "        asin_lookup = StringLookup(vocabulary = ratings_dataset['asin'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "    # creating item embedding\n",
    "        item_input = Input(shape=[1], name=\"Item-Input\" , dtype= object)\n",
    "        item_input1 = asin_lookup(item_input)\n",
    "        item_embedding = Embedding(n_items+1, 16, name=\"Item-Embedding\")(item_input1) # fourth root of unique values\n",
    "        item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "\n",
    "    # creating user embedding\n",
    "        user_input = Input(shape=[1], name=\"User-Input\" , dtype= object)\n",
    "        user_input1 = userId_lookup(user_input)\n",
    "        user_embedding = Embedding(n_users+1, 16, name=\"User-Embedding\")(user_input1)\n",
    "        user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "\n",
    "        prod = Dot(name=\"Dot-Product\", axes=1)([item_vec, user_vec])\n",
    "\n",
    "    # concatenate features\n",
    "        conc = Concatenate()([item_vec, user_vec])\n",
    "\n",
    "    # add fully-connected-layers\n",
    "        fc1 = Dense(32, activation='relu')(conc)\n",
    "\n",
    "        fc1_bn = BatchNormalization(name='batch-norm-1')(fc1)\n",
    "        fc1_dropout = Dropout(0.5)(fc1_bn)\n",
    "\n",
    "\n",
    "        pred_mlp = Dense(16, name='pred-mlp', activation='relu')(fc1_dropout)\n",
    "\n",
    "        fc3 = Concatenate()([prod, pred_mlp])\n",
    "\n",
    "        out = Dense(1 , activation='relu' )(fc3)\n",
    "\n",
    "    # Create model and compile it\n",
    "        model2 = Model([user_input, item_input], out)\n",
    "        model2.compile(optimizer=keras.optimizers.Adam(lr=0.01), loss= tf.keras.losses.mean_absolute_error  , metrics= tf.keras.metrics.RootMeanSquaredError(\n",
    "            name=\"root_mean_squared_error\", dtype=None))\n",
    "        labels = train['rating']\n",
    "        \n",
    "        \n",
    "        history = model2.fit([train.userId, train.asin], labels, batch_size=64  , epochs=15 ,  verbose=1)\n",
    "        model2.save(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\embedding_model.h5')\n",
    "    \n",
    "    item_data = np.array(list(set(ratings_dataset.asin)))\n",
    "    user = np.array([userId for i in range(len(item_data))])\n",
    "    predictions = model2.predict([user, item_data])\n",
    "    \n",
    "    predictions = np.array([a[0] for a in predictions])\n",
    "\n",
    "    recommended_indices = (-predictions).argsort()\n",
    "    item_data[recommended_indices]\n",
    "#     img_show_indices(recommended_indices[0:10])\n",
    "    a = df3[df3['userId'] == userId]['asin']\n",
    "    arr =  np.array(df2[df2['asin'].isin(item_data[recommended_indices]) & df2['asin'].isin(a)==0]['asin'])\n",
    "    arr= arr[0:numItems]\n",
    "    return arr\n",
    "#     predictions = model2.predict([user, item_data])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d90a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgRecomm(asin , numItems = 5  ):\n",
    "    if (os.path.exists(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim3.pkl')) :\n",
    "        with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim3.pkl','rb') as f:\n",
    "            cosine_sim3 = pickle.load(f)\n",
    "    else:\n",
    "        img_width, img_height, _ = 224, 224, 3 #load_image(df.iloc[0].image).shape\n",
    "\n",
    "        # Pre-Trained Model\n",
    "        base_model = ResNet50(weights='imagenet', \n",
    "                              include_top=False, \n",
    "                              input_shape = (img_width, img_height, 3))\n",
    "        base_model.trainable = False\n",
    "\n",
    "        # Add Layer Embedding\n",
    "        model = keras.Sequential([\n",
    "            base_model,\n",
    "            GlobalMaxPooling2D()\n",
    "        ])\n",
    "        df_sample      = df2\n",
    "        map_embeddings = df_sample['asin'].apply(lambda img: get_embedding1(model, img))\n",
    "        df_embs        = map_embeddings.apply(pd.Series)\n",
    "        cosine_sim3 = cosine_similarity(df_embs,df_embs)\n",
    "        with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim3.pkl','wb') as f:\n",
    "            pickle.dump(cosine_sim3, f)\n",
    "    indices3 = pd.Series(df2.index, index=df2['asin'])\n",
    "    \n",
    "    return (get_recommendations( asin, cosine_sim3 , indices3 , numItems ))\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d688f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybridRecomm ( userId = 0  ):\n",
    "    \n",
    "    \n",
    "    if  userId not in df3['userId'].values :\n",
    "        #give top_n model\n",
    "        a = topRecom( 30 )\n",
    "        _, idx = np.unique(a, return_index=True)\n",
    "        a = a[np.sort(idx)]\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    #calculate numbers of rating done by the user\n",
    "    \n",
    "    allUsersRatings = df3.groupby('userId')['userId'].count()\n",
    "    userRating = allUsersRatings[userId]\n",
    "    \n",
    "    #Determining users items \n",
    "    \n",
    "    userDataFrame = df3[df3['userId']== userId ]\n",
    "    userDataFrame = userDataFrame.sort_values(by=['rating'])\n",
    "    userAsinProducts = [] \n",
    "    for index, row in userDataFrame.iterrows():\n",
    "            userAsinProducts.append(row['asin'])\n",
    "    \n",
    "    #calculating Avg of Description legnth of All products\n",
    "\n",
    "    sum1 = 0\n",
    "    for product in userAsinProducts:\n",
    "        productIndex = (df2[df2['asin']== product ]).index[0]\n",
    "        descriptionLen = len(df2[df2['asin']==product]['description'][productIndex])\n",
    "        sum1 = sum1 + descriptionLen\n",
    "    avg = sum1 / userRating\n",
    "    \n",
    "    \n",
    "    if (userRating <= 3 ) :\n",
    "        if(avg <= 400 ):\n",
    "            recomendedProducts = [] \n",
    "            numOfContentBased = round(5/ userRating)\n",
    "            for product in userAsinProducts:\n",
    "                arr1 = contentBasedDescription(product, numOfContentBased )\n",
    "                \n",
    "                arr2 = contentBasedBrand(product,numOfContentBased)\n",
    "                arr3 = imgRecomm(product,numOfContentBased)\n",
    "                recomendedProducts = np.concatenate(  (recomendedProducts ,arr1 , arr2 , arr3) , axis = None ) \n",
    "            recomendedProducts = np.concatenate((recomendedProducts, embedding(userId ,5 ) , topRecom( 10 )) , axis = None  )\n",
    "            \n",
    "            \n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "        else:\n",
    "            recomendedProducts = [] \n",
    "            numOfContentBased = round(5/ userRating)\n",
    "            for product in userAsinProducts:\n",
    "                arr1 = contentBasedDescription(product, numOfContentBased )\n",
    "                \n",
    "                arr2 = contentBasedBrand(product,numOfContentBased)\n",
    "                arr3 = imgRecomm(product,numOfContentBased)\n",
    "                recomendedProducts = np.concatenate( (recomendedProducts ,arr1 , arr2 , arr3) ,axis = None  )\n",
    "            \n",
    "            recomendedProducts =np.concatenate((recomendedProducts, embedding(userId ,10 )) , axis = None)\n",
    "            \n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "    else :\n",
    "        if(avg <= 400 ):\n",
    "            recomendedProducts = [] \n",
    "            if(userRating <= 5 ):\n",
    "                numOfContentBased = round(5/ userRating)\n",
    "            else :\n",
    "                numOfContentBased = 1 \n",
    "            x = 0\n",
    "            for product in userAsinProducts:\n",
    "                if (x == 5 ): \n",
    "                    break \n",
    "                x = x+1 \n",
    "                \n",
    "                arr1 = contentBasedDescription(product, numOfContentBased )      \n",
    "                arr2 = contentBasedBrand(product,numOfContentBased)\n",
    "                arr3 = imgRecomm(product,numOfContentBased)\n",
    "                recomendedProducts = np.concatenate( (recomendedProducts ,arr1 , arr2 , arr3) , axis = None)\n",
    "            \n",
    "            recomendedProducts = np.concatenate((recomendedProducts ,embedding(userId,15  ) , collabrative (userId ,5)) , axis = None )\n",
    "            \n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "        else:\n",
    "            recomendedProducts = [] \n",
    "            if(userRating <= 5 ):\n",
    "                numOfContentBased = round(5/ userRating)\n",
    "            else :\n",
    "                numOfContentBased = 1 \n",
    "            x = 0\n",
    "            for product in userAsinProducts:\n",
    "                if (x == 5 ): \n",
    "                    break \n",
    "                x = x+1 \n",
    "                \n",
    "                arr1 = contentBasedDescription(product, numOfContentBased )      \n",
    "                arr2 = contentBasedBrand(product,numOfContentBased)\n",
    "                \n",
    "                recomendedProducts = np.concatenate( (recomendedProducts , arr1 , arr2) , axis = None )\n",
    "            \n",
    "           \n",
    "            recomendedProducts = np.concatenate((recomendedProducts , embedding(userId,10  ) , collabrative (userId ,5)) , axis = None )\n",
    "\n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bfd637a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933.043598743765"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating Averge description legnth of all items\n",
    "\n",
    "userAsinProducts = df2['asin'].to_numpy()\n",
    "sum1 = 0\n",
    "for product in userAsinProducts:\n",
    "    productIndex = (df2[df2['asin']== product ]).index[0]\n",
    "    descriptionLen = len(df2[df2['asin']==product]['description'][productIndex])\n",
    "    sum1 = sum1 + descriptionLen\n",
    "avg = sum1 / len(df2['asin'].to_numpy())\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fcd910f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Abdo\\\\Desktop\\\\last year\\\\project\\\\datasets\\\\last datataset isa\\\\cosine_sim1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15812\\991158078.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhybridRecomm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A2HM2RMX1CBZGM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15812\\3095893106.py\u001b[0m in \u001b[0;36mhybridRecomm\u001b[1;34m(userId)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mnumOfContentBased\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0muserRating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mproduct\u001b[0m \u001b[1;32min\u001b[0m \u001b[0muserAsinProducts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0marr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontentBasedDescription\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumOfContentBased\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0marr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontentBasedBrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumOfContentBased\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15812\\3026345934.py\u001b[0m in \u001b[0;36mcontentBasedDescription\u001b[1;34m(asin, numItems)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcosine_sim1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\cosine_sim1.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_sim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mindices1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'asin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Abdo\\\\Desktop\\\\last year\\\\project\\\\datasets\\\\last datataset isa\\\\cosine_sim1.pkl'"
     ]
    }
   ],
   "source": [
    "hybridRecomm('A2HM2RMX1CBZGM')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e81003",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = hybridRecomm('A2HM2RMX1CBZGM')\n",
    "for asin in arr:\n",
    "    img_show_asin(asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "userDataFrame = df3[df3['userId']== 'A2HM2RMX1CBZGM' ]\n",
    "userDataFrame = userDataFrame.sort_values(by=['rating'])\n",
    "userAsinProducts = [] \n",
    "for index, row in userDataFrame.iterrows():\n",
    "    userAsinProducts.append(row['asin'])\n",
    "for asin in userAsinProducts:\n",
    "    img_show_asin(asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5a9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd83f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae911e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74985e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
