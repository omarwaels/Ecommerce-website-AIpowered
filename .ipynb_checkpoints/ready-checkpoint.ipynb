{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb19428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from ast import literal_eval\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense , Concatenate , StringLookup, BatchNormalization,Dropout\n",
    "from keras.models import Model , load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from surprise import Reader, Dataset, SVD , dump\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import os # accessing directory structure\n",
    "import keras\n",
    "from keras import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "import cv2\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import requests\n",
    "import PIL.Image as Image\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e5b18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\notFound2_metadata_votecount5407.csv'\n",
    "ratingPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\notFound2_ratings5407.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b1e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_indices( product_indices):\n",
    "    fig, axs = plt.subplots(1, 10,figsize=(18,4))\n",
    "    fig.suptitle('You may also like these products', size = 22)\n",
    "    i = 0\n",
    "    for ind in product_indices:\n",
    "        response = requests.get(literal_eval(df2['imageURLHighRes'][ind])[0])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis(\"off\")\n",
    "        axs[i].set_title('{}  '.format(round(df2['vote_average'][ind],2)),y=-0.18,color=\"red\",fontsize=18)\n",
    "        i = i+1\n",
    "        fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8699f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_asin(asin):\n",
    "    response = requests.get(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "    plt.title(label= '{}  '.format(round(df2[df2['asin'] == asin]['vote_average'].values[0],2)) ,\n",
    "          loc=\"right\",\n",
    "          color='red' , fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45082687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(asin, cosine_sim , indices):\n",
    "    \n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # list of tuples [(0,1) , (1,0.5)]\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11]  # don't take 0 as we don't take the score with the same product\n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da355bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_recommendation(userId , top_n):\n",
    "    for asin in top_n[userId]:\n",
    "        img_show_asin(asin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b228dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the input is new to the dataset and not yet calculate its cosine with other products\n",
    "def tf_idftransformer (asin) :\n",
    "    x = tfidf.transform(  df2[df2['asin']== asin]['description'] + \" \" +df2[df2['asin']== asin]['feature'] )\n",
    "    cosine_sim = cosine_similarity(x, tfidf_matrix)\n",
    "    #cosine_sim1 = cosine_sim1 + cosine_sim  \n",
    "    indices = pd.Series(df2.index, index=df2['asin'])\n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0cc85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transformer (asin) :\n",
    "    x = count.transform(  df2[df2['asin']== asin]['brand'] + \" \" +df2[df2['asin']== asin]['category'] )\n",
    "    cosine_sim = cosine_similarity(x, count_matrix)\n",
    "    indices = pd.Series(df2.index, index=df2['asin'])\n",
    "    img_show_asin(asin)\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:11] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    return df2['asin'].iloc[product_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a39fdc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "363e40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the products that has not been rated yet\n",
    "def build_anti_testset(self, userId, fill=None ):\n",
    "        \"\"\"Return a list of ratings that can be used as a testset in the\n",
    "        :meth:`test() <surprise.prediction_algorithms.algo_base.AlgoBase.test>`\n",
    "        method.\n",
    "\n",
    "        The ratings are all the ratings that are **not** in the trainset, i.e.\n",
    "        all the ratings :math:`r_{ui}` where the user :math:`u` is known, the\n",
    "        item :math:`i` is known, but the rating :math:`r_{ui}`  is not in the\n",
    "        trainset. As :math:`r_{ui}` is unknown, it is either replaced by the\n",
    "        :code:`fill` value or assumed to be equal to the mean of all ratings\n",
    "        :meth:`global_mean <surprise.Trainset.global_mean>`.\n",
    "\n",
    "        Args:\n",
    "            fill(float): The value to fill unknown ratings. If :code:`None` the\n",
    "                global mean of all ratings :meth:`global_mean\n",
    "                <surprise.Trainset.global_mean>` will be used.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples ``(uid, iid, fill)`` where ids are raw ids.\n",
    "        \"\"\"\n",
    "        fill = self.global_mean if fill is None else float(fill)\n",
    "\n",
    "        anti_testset = []\n",
    "        user_items = {j for (j, _) in self.ur[userId]}\n",
    "        anti_testset += [\n",
    "                (self.to_raw_uid(real_trainset.to_inner_uid(userId)), self.to_raw_iid(i), fill)\n",
    "                for i in self.all_items()\n",
    "                if i not in user_items\n",
    "            ]\n",
    "        return anti_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af8a1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh (num , thresh = 2.5):\n",
    "    if num >  thresh:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a88f719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C= df2['vote_average'].mean()\n",
    "m= df2['vote_count'].quantile(0.9)\n",
    "df22 = df2.copy().loc[df2['vote_count'] >= m]\n",
    "\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d1a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(dataPath)\n",
    "df3= pd.read_csv(ratingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d37d4c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topRecom(numPerCategory = 5 ) :\n",
    "    \n",
    "    dataFramePhone = df22[df22['category'].str[2:27] == 'Cell Phones & Accessories']\n",
    "\n",
    "    dataFrameAppliances = df22[df22['category'].str[2:12] == 'Appliances']\n",
    "    dataFrameSports = df22[df22['category'].str[2:19] == 'Sports & Outdoors']\n",
    "    dataFramePhone['score'] = dataFramePhone.apply(weighted_rating, axis=1)\n",
    "\n",
    "    dataFrameAppliances['score'] = dataFrameAppliances.apply(weighted_rating, axis=1)\n",
    "    dataFrameSports['score'] = dataFrameSports.apply(weighted_rating, axis=1)\n",
    "    \n",
    "    dataFramePhone = dataFramePhone.sort_values('score', ascending=False)\n",
    "    dataFrameAppliances = dataFrameAppliances.sort_values('score', ascending=False)\n",
    "    dataFrameSports = dataFrameSports.sort_values('score', ascending=False)\n",
    "    \n",
    "    arr1 = np.array(dataFramePhone['asin'][0:numPerCategory])\n",
    "    arr2 = np.array(dataFrameAppliances['asin'][0:numPerCategory])\n",
    "    arr3 = np.array(dataFrameSports['asin'][0:numPerCategory])\n",
    "    \n",
    "    coldstart = np.concatenate((arr1 , arr2 , arr3))\n",
    "\n",
    "    return coldstart\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5a4e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedDescription(asin  ):\n",
    "    df2= pd.read_csv(dataPath)\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim1.pkl'):\n",
    "        with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim1.pkl','rb') as f:\n",
    "            cosine_sim1 = pickle.load(f)\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english') # playing--->play\n",
    "        analyzer = TfidfVectorizer().build_analyzer()\n",
    "        def stemmed_words(doc):\n",
    "            return (stemmer.stem(w) for w in analyzer(doc))\n",
    "        tfidf = TfidfVectorizer( analyzer = stemmed_words ,\n",
    "                                 ngram_range=(1,1),\n",
    "                                 min_df=3/31858, # if the word is repeated less than three times then I dont care about it\n",
    "                                 max_df=0.5, # if the word is repeated more than 50% then I don't care about it\n",
    "                                 stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(  df2['description'] + \" \" + df2['feature'] )\n",
    "        cosine_sim1 = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim1.pkl','wb') as f:\n",
    "            pickle.dump(cosine_sim1, f)\n",
    "    indices1 = pd.Series(df2.index, index=df2['asin'])\n",
    "    get_recommendations(asin , cosine_sim1  ,indices1)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad4aa0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentBasedBrand(asin = \"B0006TIA8Y\"  ):\n",
    "    df2= pd.read_csv(dataPath)\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim2.pkl'):\n",
    "        with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim2.pkl','rb') as f:\n",
    "            cosine_sim2 = pickle.load(f)\n",
    "    else:\n",
    "        count = CountVectorizer(stop_words='english')\n",
    "        count_matrix = count.fit_transform(df2['brand']+\" \"+ df2['category'])\n",
    "        cosine_sim2 = cosine_similarity(count_matrix,count_matrix)\n",
    "        with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim2.pkl','wb') as f:\n",
    "            pickle.dump(cosine_sim2, f)\n",
    "    indices2 = pd.Series(df2.index, index=df2['asin'])\n",
    "    get_recommendations( asin, cosine_sim2 , indices2 )\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31802ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collabrative (user ):\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\dump_file'):\n",
    "        file_name = os.path.expanduser(r\"C:\\Users\\Sigma\\ecommerce\\dump_file\")\n",
    "        _,svd = dump.load(file_name)\n",
    "       \n",
    "    else:\n",
    "    \n",
    "        \n",
    "        svd = SVD(n_factors= 30 , n_epochs= 20 , lr_all = 0.005 , reg_all = 0.02 , random_state=42  )\n",
    "        file_name = os.path.expanduser(r\"C:\\Users\\Sigma\\ecommerce\\dump_file\")\n",
    "        dump.dump(file_name, algo=svd)\n",
    "\n",
    "    ratings = pd.read_csv(ratingPath)\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(ratings[['userId', 'asin', 'rating']], reader)\n",
    "    real_trainset = data.build_full_trainset()\n",
    "    svd.fit(real_trainset)\n",
    "    real_testset = build_anti_testset(real_trainset   , user )\n",
    "    predictions = svd.test(real_testset)\n",
    "    top_n = get_top_n(predictions, n=20)\n",
    "    collaborative_recommendation(user , top_n)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d414d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(user  ):\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5'):\n",
    "        model2 = load_model(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "        \n",
    "    else:\n",
    "        ratings_dataset = pd.read_csv(ratingPath)\n",
    "        train, test = train_test_split(ratings_dataset, test_size=0.2, random_state=42 , shuffle=True)\n",
    "        n_users = len(ratings_dataset['userId'].unique())\n",
    "        n_items = len(ratings_dataset['asin'].unique())\n",
    "        userId_lookup = StringLookup(vocabulary = ratings_dataset['userId'].unique(), mask_token = None )\n",
    "        asin_lookup = StringLookup(vocabulary = ratings_dataset['asin'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "        # creating item embedding\n",
    "        item_input = Input(shape=[1], name=\"Item-Input\" , dtype= object)\n",
    "        item_input1 = asin_lookup(item_input)\n",
    "        item_embedding = Embedding(n_items+1, 8, name=\"Item-Embedding\")(item_input1) # fourth root of unique values\n",
    "        item_vec = Flatten(name=\"Flatten-Items\")(item_embedding)\n",
    "\n",
    "        # creating user embedding\n",
    "        user_input = Input(shape=[1], name=\"User-Input\" , dtype= object)\n",
    "        user_input1 = userId_lookup(user_input)\n",
    "        user_embedding = Embedding(n_users+1, 8, name=\"User-Embedding\")(user_input1)\n",
    "        user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)\n",
    "\n",
    "\n",
    "        prod = Dot(name=\"Dot-Product\", axes=1)([item_vec, user_vec])\n",
    "\n",
    "        # concatenate features\n",
    "        conc = Concatenate()([item_vec, user_vec])\n",
    "\n",
    "        # add fully-connected-layers\n",
    "        fc1 = Dense(64, activation='relu')(conc)\n",
    "\n",
    "        fc1_bn = BatchNormalization(name='batch-norm-1')(fc1)\n",
    "        fc1_dropout = Dropout(0.5)(fc1_bn)\n",
    "\n",
    "        fc2 = Dense(16, activation='relu')(fc1_dropout)\n",
    "\n",
    "        fc2_bn = BatchNormalization(name='batch-norm-2')(fc2)\n",
    "        fc2_dropout = Dropout(0.5)(fc2_bn)\n",
    "\n",
    "        pred_mlp = Dense(8, name='pred-mlp', activation='relu')(fc2_dropout)\n",
    "\n",
    "        fc3 = Concatenate()([prod, pred_mlp])\n",
    "\n",
    "        out = Dense(1 , activation='sigmoid' )(fc3)\n",
    "\n",
    "        # Create model and compile it\n",
    "        model2 = Model([user_input, item_input], out)\n",
    "        model2.compile(optimizer='adam', loss='binary_crossentropy'  , metrics= ['accuracy'])\n",
    "        \n",
    "        labels = train['rating'].apply(thresh)\n",
    "        \n",
    "        \n",
    "        history = model2.fit([train.userId, train.asin], labels, batch_size=64  , epochs=8 ,  verbose=1)\n",
    "        model2.save(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "#     predictions = model2.predict([user, item_data])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688f7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e81003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
