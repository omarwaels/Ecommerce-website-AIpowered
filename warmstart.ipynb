{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4a7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from ast import literal_eval\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import warnings\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense , Concatenate , StringLookup, BatchNormalization,Dropout \n",
    "from keras.models import Model , load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import warmstart_embedding_matrix\n",
    "\n",
    "dataPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\allData.csv'\n",
    "ratingPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\both3.csv'\n",
    "new = pd.read_csv(r'C:\\Users\\Sigma\\ecommerce\\new.csv')\n",
    "model2 = 1\n",
    "\n",
    "ratings_dataset = pd.read_csv(ratingPath)\n",
    "n_users = len(ratings_dataset['userID'].unique())\n",
    "n_items = len(ratings_dataset['itemID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6719fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    userId_lookup = StringLookup(vocabulary = ratings_dataset['userID'].unique(), mask_token = None )\n",
    "    asin_lookup = StringLookup(vocabulary = ratings_dataset['itemID'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "    item_input = Input(shape=[1],name='item-input'  , dtype= object)\n",
    "    user_input = Input(shape=[1], name='user-input' , dtype= object)\n",
    "\n",
    "    # MLP Embeddings\n",
    "    item_input1 = asin_lookup(item_input)\n",
    "    user_input1 = userId_lookup(user_input)\n",
    "\n",
    "    item_embedding_mlp = Embedding(n_items + 1, 16, name='item-embedding-mlp'  )(item_input1)\n",
    "    item_vec_mlp = Flatten(name='flatten-item-mlp')(item_embedding_mlp)\n",
    "\n",
    "    user_embedding_mlp = Embedding(n_users + 1, 16, name='user-embedding-mlp' )(user_input1)\n",
    "    user_vec_mlp = Flatten(name='flatten-user-mlp')(user_embedding_mlp)\n",
    "\n",
    "    # MF Embeddings\n",
    "    item_embedding_mf = Embedding(n_items + 1, 16, name='item-embedding-mf' , embeddings_regularizer='L2' )(item_input1)\n",
    "    item_vec_mf = Flatten(name='flatten-item-mf')(item_embedding_mf)\n",
    "\n",
    "    user_embedding_mf = Embedding(n_users + 1, 16, name='user-embedding-mf' , embeddings_regularizer='L2' )(user_input1)\n",
    "    user_vec_mf = Flatten(name='flatten-user-mf')(user_embedding_mf)\n",
    "\n",
    "    # MLP layers\n",
    "\n",
    "    concat = Concatenate()([item_vec_mlp, user_vec_mlp])\n",
    "    concat_dropout = Dropout(0.5)(concat)\n",
    "\n",
    "    fc_1 = Dense(128, name='fc-1', activation='relu')(concat_dropout)\n",
    "    fc_1_bn = BatchNormalization(name='batch-norm-1')(fc_1)\n",
    "    fc_1_dropout = Dropout(0.5)(fc_1_bn)\n",
    "\n",
    "    fc_2 = Dense(64, name='fc-2', activation='relu')(fc_1_dropout)\n",
    "    fc_2_bn = BatchNormalization(name='batch-norm-2')(fc_2)\n",
    "    fc_2_dropout = Dropout(0.5)(fc_2_bn)\n",
    "\n",
    "\n",
    "    # fc_3 = Dense(32, name='fc-3', activation='relu')(fc_2_dropout)\n",
    "    # fc_3_bn = BatchNormalization(name='batch-norm-3')(fc_3)\n",
    "    # fc_3_dropout = Dropout(0.5)(fc_3_bn)\n",
    "\n",
    "\n",
    "    # Prediction from both layers\n",
    "    pred_mlp = Dense(16, name='pred-mlp', activation='relu' )(fc_2_dropout)\n",
    "\n",
    "\n",
    "\n",
    "    pred_mf = Dot(name=\"Dot-Product\", axes=1)([item_vec_mf, user_vec_mf])\n",
    "    combine_mlp_mf = Concatenate()([pred_mf, pred_mlp])\n",
    "\n",
    "    # Final prediction\n",
    "    result = Dense(1, name='result', activation='relu' )(combine_mlp_mf)\n",
    "\n",
    "    model2 = Model([user_input, item_input], result)\n",
    "    model2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss= tf.keras.losses.mean_absolute_error  , metrics= tf.keras.metrics.RootMeanSquaredError(\n",
    "        name=\"root_mean_squared_error\", dtype=None))\n",
    "    import os\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5'):\n",
    "        model2 = load_model(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "    else:\n",
    "        #history = model2.fit([train.userId, train.asin], labels, batch_size=64  , epochs=10 ,  verbose=1)\n",
    "        history = model2.fit([ratings_dataset.userID, ratings_dataset.itemID], ratings_dataset['rating'] , batch_size=64  , epochs= 10 ,  verbose=1 )\n",
    "        model2.save(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "    print(len(model2.layers[15].get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eceb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319fba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newEmbedModel() :\n",
    "    \n",
    "    import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.utils import warmstart_embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    base_vocabulary_user = ratings_dataset['userID'].unique()\n",
    "    base_vocabulary_item = ratings_dataset['itemID'].unique()\n",
    "    # old_text_vectorization_layer.get_vocabulary()\n",
    "    new = pd.read_csv(r'C:\\Users\\Sigma\\ecommerce\\new.csv')\n",
    "    new_vocabulary_user = new['userID'].unique()\n",
    "    new_vocabulary_item = new['itemID'].unique()\n",
    "    # new_text_vectorization_layer.get_vocabulary()\n",
    "    # get previous embedding layer weights\n",
    "\n",
    "\n",
    "    embedding_weights_base_item1 = model2.get_layer('item-embedding-mlp').get_weights()[0]\n",
    "    embedding_weights_base_item2 = model2.get_layer('item-embedding-mf').get_weights()[0]\n",
    "    embedding_weights_base_user1 = model2.get_layer('user-embedding-mlp').get_weights()[0]\n",
    "    embedding_weights_base_user2 = model2.get_layer('user-embedding-mf').get_weights()[0]\n",
    "\n",
    "\n",
    "\n",
    "    warmstarted_embedding1 = warmstart_embedding_matrix(\n",
    "                                  base_vocabulary_user,\n",
    "                                  new_vocabulary_user,\n",
    "                                  base_embeddings=embedding_weights_base_user1,\n",
    "                                  new_embeddings_initializer=\"uniform\")\n",
    "    updated_embedding_variable1 = tf.Variable(warmstarted_embedding1)\n",
    "\n",
    "    # update embedding layer weights ####################################33\n",
    "    model2.layers[5].embeddings = updated_embedding_variable1\n",
    "\n",
    "\n",
    "    warmstarted_embedding2 = warmstart_embedding_matrix(\n",
    "                                  base_vocabulary_user,\n",
    "                                  new_vocabulary_user,\n",
    "                                  base_embeddings=embedding_weights_base_user2,\n",
    "                                  new_embeddings_initializer=\"uniform\")\n",
    "    updated_embedding_variable2 = tf.Variable(warmstarted_embedding2)\n",
    "\n",
    "    # update embedding layer weights  ####################################\n",
    "    model2.layers[15].embeddings = updated_embedding_variable2\n",
    "\n",
    "\n",
    "    warmstarted_embedding3 = warmstart_embedding_matrix(\n",
    "                                  base_vocabulary_item,\n",
    "                                  new_vocabulary_item,\n",
    "                                  base_embeddings=embedding_weights_base_item1,\n",
    "                                  new_embeddings_initializer=\"uniform\")\n",
    "    updated_embedding_variable3 = tf.Variable(warmstarted_embedding3)\n",
    "\n",
    "    # update embedding layer weights  ####################################\n",
    "    model2.layers[4].embeddings = updated_embedding_variable3\n",
    "\n",
    "\n",
    "    warmstarted_embedding4 = warmstart_embedding_matrix(\n",
    "                                  base_vocabulary_item,\n",
    "                                  new_vocabulary_item,\n",
    "                                  base_embeddings=embedding_weights_base_item2,\n",
    "                                  new_embeddings_initializer=\"uniform\")\n",
    "    updated_embedding_variable4 = tf.Variable(warmstarted_embedding4)\n",
    "\n",
    "    # update embedding layer weights\n",
    "    model2.layers[14].embeddings = updated_embedding_variable4\n",
    "\n",
    "    new5 = pd.read_csv(r'C:\\Users\\Sigma\\ecommerce\\new1.csv')\n",
    "    history = model2.fit([new5.userID, new5.itemID], new5['rating'] , batch_size=64  , epochs=35,  verbose=1 )\n",
    "    new5 = new5[0:0]\n",
    "    new5.to_csv(r'C:\\Users\\Sigma\\ecommerce\\new1.csv' , index=False)\n",
    "    new.to_csv(r'C:\\Users\\Sigma\\ecommerce\\both3.csv' , index=False)\n",
    "    print(len(model2.layers[15].get_weights()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e67fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc7f8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12712"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = load_model(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "len(model2.get_layer('user-embedding-mlp').get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa61f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 634, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1166, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1213, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 216, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable Variable:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9836\\683209020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnewEmbedModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9836\\3141755556.py\u001b[0m in \u001b[0;36mnewEmbedModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mnew5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Sigma\\ecommerce\\new1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muserID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew5\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mnew5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew5\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mnew5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Sigma\\ecommerce\\new1.csv'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 634, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1166, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1213, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"C:\\Users\\Sigma\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 216, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable Variable:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'\n"
     ]
    }
   ],
   "source": [
    "newEmbedModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9d72e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
