{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9692296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from ast import literal_eval\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense , Concatenate , StringLookup, BatchNormalization,Dropout\n",
    "from keras.models import Model , load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from surprise import Reader, Dataset, SVD , dump\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import matplotlib.image as mpimg\n",
    "import os # accessing directory structure\n",
    "import keras\n",
    "from keras import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "import cv2\n",
    "import urllib.request\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import requests\n",
    "import PIL.Image as Image\n",
    "import io\n",
    "from tensorflow.keras.utils import warmstart_embedding_matrix\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense , Concatenate , StringLookup, BatchNormalization,Dropout \n",
    "from keras.models import Model , load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from recbole.model.sequential_recommender import BERT4Rec \n",
    "import pandas as pd\n",
    "from recbole.evaluator.metrics import RMSE , MAE , Hit , NDCG , Recall , MRR\n",
    "import logging\n",
    "from logging import getLogger\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset , data_preparation\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from recbole.evaluator.metrics import Hit , NDCG , Recall\n",
    "from recbole.quick_start import run_recbole , load_data_and_model\n",
    "import torch\n",
    "from recbole.data.interaction import Interaction\n",
    "import csv\n",
    "import datetime\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#end\n",
    "\n",
    "# imports API\n",
    "\n",
    "from flask import Flask\n",
    "from flask_restful import Resource , Api\n",
    "from flask import request, render_template\n",
    "from flask_cors import CORS\n",
    "import json as json\n",
    "import csv\n",
    "\n",
    "#end\n",
    "\n",
    "\n",
    "# Abdelrhman amr part \n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarabic.araby as ar\n",
    "\n",
    "import re , emoji, functools, operator, string\n",
    "import torch, gc, random, os\n",
    "import string \n",
    "import nltk\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"clothes_home_processed.csv\")\n",
    "train_en=df['review']\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"arabic_review_processed.csv\")\n",
    "train_ar=df['review']\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "train_en[8]\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "train_en\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "def lower_text(text):\n",
    "    text = \" \".join(i.lower() for i in text.split())\n",
    "    return text\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_noise(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(\"\\[[^]]*\\]\", \"\", text)\n",
    "    return text\n",
    "def remove_punc(text):\n",
    "    tokens = text.split()\n",
    "    re_punc = re.compile(\"[%s]\"%re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub(\"\", w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "def preprocess_data_en(text):\n",
    "    text = lower_text(text)\n",
    "    text = remove_noise(text)\n",
    "    text = remove_punc(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def get_emoji_regexp():\n",
    "    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "    pattern = u'(' + u'|'.join(re.escape(u) for u in emojis) + u')'\n",
    "    return re.compile(pattern)\n",
    "def preprocess_data_ar (text):\n",
    "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)\n",
    "  text = re.sub(r\"https\\S+\", \"\", text)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = re.sub(\"(\\s\\d+)\",\"\",text) \n",
    "  text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n",
    "  text = re.sub(\"\\d+\", \" \", text)\n",
    "  text = ar.strip_tashkeel(text)\n",
    "  text = ar.strip_tatweel(text)\n",
    "  text = text.replace(\"#\", \" \");\n",
    "  text = text.replace(\"@\", \" \");\n",
    "  text = text.replace(\"_\", \" \");\n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "  text = text.translate(translator)\n",
    "  em = text\n",
    "  em_split_emoji = get_emoji_regexp().split(em)\n",
    "  em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "  em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "  text = \" \".join(em_split)\n",
    "  text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "  text = text.replace(\"آ\", \"ا\")\n",
    "  text = text.replace(\"إ\", \"ا\")\n",
    "  text = text.replace(\"أ\", \"ا\")\n",
    "  text = text.replace(\"ؤ\", \"و\")\n",
    "  text = text.replace(\"ئ\", \"ي\")\n",
    "   \n",
    "  return text\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "PRETRAINED_LM = \"bert-base-uncased\"\n",
    "btokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)\n",
    "\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "def encode(docs):\n",
    "    '''\n",
    "    This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "    '''\n",
    "    encoded_dict = btokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                            return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "model_en = BertForSequenceClassification.from_pretrained('bert_momken')\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#model_en.load_state_dict(torch.load(\"Bert_trained_clothes_home_fin\"))\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "device\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "tokenizer_ar = Tokenizer(num_words=5000, oov_token=\"<00V>\")\n",
    "tokenizer_ar.fit_on_texts(train_ar)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "model_ar=tf.keras.models.load_model(\"sentiment_analysis_arabic\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "def getSentiment(text):   \n",
    "    if(isEnglish(text)):\n",
    "        model=model_en\n",
    "        text=preprocess_data_en(text)\n",
    "        data={\"review\":[text],\"sentiment\":[0]}\n",
    "        df12= pd.DataFrame(data);\n",
    "        test_input_ids, test_att_masks = encode(df12['review'].values.tolist())\n",
    "        test_y = torch.LongTensor(df12['sentiment'].values.tolist())\n",
    "        test_dataset = TensorDataset(test_input_ids, test_att_masks, test_y)\n",
    "        test_sampler = SequentialSampler(test_dataset)\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=16)\n",
    "        with torch.no_grad():\n",
    "            for step_num, batch_data in tqdm(enumerate(test_dataloader)):\n",
    "                input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                output = model(input_ids = input_ids, attention_mask=att_mask, labels= labels)\n",
    "                pred = np.argmax(output.logits.cpu().detach().numpy())\n",
    "    else:\n",
    "        model=model_ar\n",
    "        text=preprocess_data_ar(text)\n",
    "        tokenizer1=tokenizer_ar\n",
    "        text_seq = tokenizer1.texts_to_sequences([(text)])\n",
    "        text_seq = pad_sequences(text_seq, maxlen=100)\n",
    "        pred = round(model.predict(text_seq)[0][0])\n",
    "\n",
    "    return pred\n",
    "    \n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  end\n",
    "\n",
    "#Recomedation blocks\n",
    "dataPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\allData.csv'\n",
    "ratingPath = 'C:\\\\Users\\\\Sigma\\\\ecommerce\\\\both3.csv'\n",
    "df2= pd.read_csv(dataPath)\n",
    "df3= pd.read_csv(ratingPath)\n",
    "\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(df2['category'])\n",
    "\n",
    "stemmer = SnowballStemmer('english') # playing--->play\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "tfidf = TfidfVectorizer( analyzer = stemmed_words ,\n",
    "                         ngram_range=(1,1),\n",
    "                         min_df=3/31858, # if the word is repeated less than three times then I dont care about it\n",
    "                         max_df=0.5, # if the word is repeated more than 50% then I don't care about it\n",
    "                         stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(  df2['feature']  )\n",
    "\n",
    "img_width, img_height, _ = 224, 224, 3 #load_image(df.iloc[0].image).shape\n",
    "\n",
    "# Pre-Trained Model\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape = (img_width, img_height, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add Layer Embedding\n",
    "model_img = keras.Sequential([\n",
    "    base_model,\n",
    "    GlobalMaxPooling2D()\n",
    "])\n",
    "# Add Layer Embedding\n",
    "\n",
    "with open(r'C:\\Users\\Sigma\\ecommerce\\images_emb.pkl','rb') as f:\n",
    "                df_embs = pickle.load(f)\n",
    "\n",
    "ggg = df3.sort_values(by=['timestamp'])\n",
    "ggg = ggg.rename(columns={'userID': 'user_id:token', 'itemID': 'item_id:token', 'rating': 'rating:token' ,'timestamp': 'timestamp:float'})\n",
    "ggg.to_csv(r'C:\\Users\\Sigma\\ecommerce\\old&new_condition.inter', index=False, sep='\\t')\n",
    "\n",
    "# this cell is made to empty the new_all\n",
    "empty = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\new_all.csv\")\n",
    "empty = empty[0:0]\n",
    "empty.to_csv(r'C:\\Users\\Sigma\\ecommerce\\new_all.csv', index=False)\n",
    "\n",
    "def img_show_asin(asin , x=0):\n",
    "    if x == 1:\n",
    "        img = cv2.imread('C:\\\\Users\\\\Sigma\\\\ecommerce\\\\images' + \"\\\\\" + asin + '.jpg')\n",
    "    else:\n",
    "        response = requests.get(literal_eval(df2[df2['asin'] == asin]['imageURLHighRes'].values[0])[0])\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "    plt.title(label= '{}  '.format(round(df2[df2['asin'] == asin]['vote_average'].values[0],2)) ,\n",
    "          loc=\"right\",\n",
    "          color='red' , fontsize=18)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_recommendations(asin, cosine_sim , indices , numItems):\n",
    "    \n",
    "\n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx])) # list of tuples [(0,1) , (1,0.5)]\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[1:numItems+1]  # don't take 0 as we don't take the score with the same product\n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    arr =  np.array(df2['asin'].iloc[product_indices])\n",
    "    \n",
    "    return (arr)\n",
    "\n",
    "\n",
    "    \n",
    "def get_embedding1(model, asin):\n",
    "    # Reshape \n",
    "    img = cv2.imread('C:\\\\Users\\\\Sigma\\\\ecommerce\\\\images' + \"\\\\\" + asin + '.jpg')\n",
    "        \n",
    "    if img.shape[2] == 1:\n",
    "        img = cv2.merge((img,img,img))   \n",
    "    \n",
    "    \n",
    "    #img = tf.keras.utils.load_img(img, target_size=(img_width, img_height))\n",
    "    \n",
    "    width = 224\n",
    "    height = 224\n",
    "    \n",
    "    dsize = (width, height)\n",
    "    output = cv2.resize(img, dsize)\n",
    "    \n",
    "    # img to Array\n",
    "    x   = img_to_array(output)\n",
    "    # Expand Dim (1, w, h)\n",
    "    x   = np.expand_dims(x, axis=0)\n",
    "    # Pre process Input\n",
    "    x   = preprocess_input(x)\n",
    "    return model.predict(x , verbose = 1).reshape(-1)\n",
    "\n",
    "def get_upload_img(model, name):\n",
    "    # Reshape \n",
    "    img = cv2.imread(name)\n",
    "        \n",
    "    if img.shape[2] == 1:\n",
    "        img = cv2.merge((img,img,img))   \n",
    "    \n",
    "    \n",
    "    #img = tf.keras.utils.load_img(img, target_size=(img_width, img_height))\n",
    "    \n",
    "    width = 224\n",
    "    height = 224\n",
    "    \n",
    "    dsize = (width, height)\n",
    "    output = cv2.resize(img, dsize)\n",
    "    \n",
    "    # img to Array\n",
    "    x   = img_to_array(output)\n",
    "    # Expand Dim (1, w, h)\n",
    "    x   = np.expand_dims(x, axis=0)\n",
    "    # Pre process Input\n",
    "    x   = preprocess_input(x)\n",
    "    return model.predict(x , verbose = 1).reshape(-1)\n",
    "\n",
    "def collaborative_recommendation(userId , top_n, ratings):\n",
    "    \n",
    "    a = ratings[ratings['userID'] == userId]\n",
    "    \n",
    "    asinArr = []\n",
    "    for asin in top_n[userId]:\n",
    "        a = a[a['itemID'] == asin]\n",
    "        if (a.empty):\n",
    "            asinArr.append(asin[0])\n",
    "    return asinArr\n",
    "        \n",
    "#         img_show_asin(asin[0])\n",
    "def transformer (asin, cosine_sim , numItems) :\n",
    "\n",
    "    \n",
    "    \n",
    "    # Get the index of the movie that matches the asin number\n",
    "    #idx = indices[asin]\n",
    "\n",
    "    # Get the similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[0]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar products\n",
    "    sim_scores = sim_scores[0:numItems] \n",
    "\n",
    "    # Get the movie indices\n",
    "    product_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    #img_show_indices( product_indices)\n",
    "                \n",
    "    # Return the top 10 most similar movies\n",
    "    arr =  np.array(df2['asin'].iloc[product_indices])\n",
    "\n",
    "    \n",
    "    return (arr)\n",
    "def get_top_n(predictions, n):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "# return the products that has not been rated yet\n",
    "def build_anti_testset(self, userId, fill=None ):\n",
    "        \"\"\"Return a list of ratings that can be used as a testset in the\n",
    "        :meth:`test() <surprise.prediction_algorithms.algo_base.AlgoBase.test>`\n",
    "        method.\n",
    "\n",
    "        The ratings are all the ratings that are **not** in the trainset, i.e.\n",
    "        all the ratings :math:`r_{ui}` where the user :math:`u` is known, the\n",
    "        item :math:`i` is known, but the rating :math:`r_{ui}`  is not in the\n",
    "        trainset. As :math:`r_{ui}` is unknown, it is either replaced by the\n",
    "        :code:`fill` value or assumed to be equal to the mean of all ratings\n",
    "        :meth:`global_mean <surprise.Trainset.global_mean>`.\n",
    "\n",
    "        Args:\n",
    "            fill(float): The value to fill unknown ratings. If :code:`None` the\n",
    "                global mean of all ratings :meth:`global_mean\n",
    "                <surprise.Trainset.global_mean>` will be used.\n",
    "\n",
    "        Returns:\n",
    "            A list of tuples ``(uid, iid, fill)`` where ids are raw ids.\n",
    "        \"\"\"\n",
    "        fill = self.global_mean if fill is None else float(fill)\n",
    "\n",
    "        anti_testset = []\n",
    "        user_items = {j for (j, _) in self.ur[userId]}\n",
    "        anti_testset += [\n",
    "                (self.to_raw_uid(self.to_inner_uid(userId)), self.to_raw_iid(i), fill)\n",
    "                for i in self.all_items()\n",
    "                if i not in user_items\n",
    "            ]\n",
    "        return anti_testset\n",
    "\n",
    "\n",
    "def weighted_rating(x, m, C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "def topRecom(numPerCategory = 5 ) :\n",
    "    \n",
    "    current_db= pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\current_database.csv\")\n",
    "        \n",
    "    C= current_db['vote_average'].mean()\n",
    "    m= current_db['vote_count'].quantile(0.9)\n",
    "    current_db22 = current_db.copy().loc[current_db['vote_count'] >= m]\n",
    "    \n",
    "    dataFrameClothing = current_db22[current_db22['category'].str[2:10] == 'Clothing']\n",
    "    dataFrameHome = current_db22[current_db22['category'].str[2:10] == 'Home & K']\n",
    "    \n",
    "    dataFrameClothing['score'] = dataFrameClothing.apply(weighted_rating , args=(m,C), axis=1)\n",
    "    dataFrameHome['score'] = dataFrameHome.apply(weighted_rating ,args=(m,C) , axis=1)\n",
    "   \n",
    "    \n",
    "    dataFrameClothing = dataFrameClothing.sort_values('score', ascending=False)\n",
    "    dataFrameHome = dataFrameHome.sort_values('score', ascending=False)\n",
    "    \n",
    "    \n",
    "    arr1 = np.array(dataFrameClothing['asin'][0:round(numPerCategory/2)])\n",
    "    arr2 = np.array(dataFrameHome['asin'][0:round(numPerCategory/2)])\n",
    "    \n",
    "    \n",
    "    coldstart = np.concatenate((arr1 , arr2 ))\n",
    "\n",
    "    return coldstart\n",
    "    \n",
    "def contentBased(asin , numItems = 5 ):\n",
    "    \n",
    "    if asin in np.array(df2['asin']):\n",
    "        if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim1.pkl'):\n",
    "            with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim1.pkl','rb') as f:\n",
    "                cosine_sim1 = pickle.load(f)\n",
    "\n",
    "        else:\n",
    "\n",
    "            cosine_sim1 = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "            with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim1.pkl','wb') as f:\n",
    "                pickle.dump(cosine_sim1, f)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim4.pkl'):\n",
    "            with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim4.pkl','rb') as f:\n",
    "                cosine_sim4 = pickle.load(f)\n",
    "        else:\n",
    "            #count = CountVectorizer(stop_words='english')\n",
    "            #count_matrix = count.fit_transform(df2['category'])\n",
    "            cosine_sim4 = cosine_similarity(count_matrix,count_matrix)\n",
    "            with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim4.pkl','wb') as f:\n",
    "                pickle.dump(cosine_sim4, f)\n",
    "\n",
    " \n",
    "    else:\n",
    "        df_new= pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\current_database.csv\")\n",
    "        x = tfidf.transform(  df_new[df_new['asin']== asin]['feature'] ) #df2[df2['asin']== asin]['description'] + \" \" +\n",
    "        cosine_sim_new1 = cosine_similarity(x, tfidf_matrix)\n",
    "        x = count.transform(  df_new[df_new['asin']== asin]['category'] ) #df2[df2['asin']== asin]['brand'] + \" \" +\n",
    "        cosine_sim_new4 = cosine_similarity(x, count_matrix)\n",
    "\n",
    "    if asin in np.array(df2['asin']):\n",
    "        indices = pd.Series(df2.index, index=df2['asin']) # this must be the old data\n",
    "        featureArr = get_recommendations(asin , cosine_sim1  ,indices , round(numItems/2))\n",
    "        categoryArr = get_recommendations(asin , cosine_sim4  ,indices , round(numItems/2))\n",
    "    else:\n",
    "        featureArr = transformer (asin, cosine_sim_new1 , round(numItems/2))\n",
    "        categoryArr = transformer (asin, cosine_sim_new4 , round(numItems/2))\n",
    "        \n",
    "        \n",
    "    Total = np.concatenate(  ( categoryArr ,  featureArr) , axis = None ) \n",
    "    Total = np.unique(Total)\n",
    "\n",
    "    return (Total)\n",
    "def imgRecomm(asin , numItems = 5  ):\n",
    "    if asin in np.array(df2['asin']):\n",
    "\n",
    "        if (os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim3.pkl')) :\n",
    "            with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim3.pkl','rb') as f:\n",
    "                cosine_sim3 = pickle.load(f)\n",
    "        else:\n",
    "\n",
    "            cosine_sim3 = cosine_similarity(df_embs,df_embs)\n",
    "            with open(r'C:\\Users\\Sigma\\ecommerce\\cosine_sim3.pkl','wb') as f:\n",
    "                pickle.dump(cosine_sim3, f)\n",
    "    \n",
    "    else:\n",
    "        df_new= pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\current_database.csv\")\n",
    "        x = get_embedding1(model_img, asin).reshape(1,-1)\n",
    "        cosine_sim_new3 = cosine_similarity(x, df_embs)\n",
    "            \n",
    "    \n",
    "    \n",
    "    if asin in np.array(df2['asin']):\n",
    "        indices3 = pd.Series(df2.index, index=df2['asin'])\n",
    "        imgarr = get_recommendations( asin, cosine_sim3 , indices3 , numItems )\n",
    "    else:\n",
    "        imgarr = transformer (asin, cosine_sim_new3 , numItems)\n",
    "\n",
    "        \n",
    "    \n",
    "    return (imgarr)\n",
    "\n",
    "\n",
    "def embedding(userId , numItems = 5 ):\n",
    "    \n",
    "    ratings_dataset = pd.read_csv(ratingPath)\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5'):\n",
    "        model2 = load_model(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "#         train, test = train_test_split(ratings_dataset, test_size=0.2, random_state=42 , shuffle=True)\n",
    "        n_users = len(ratings_dataset['userID'].unique())\n",
    "        n_items = len(ratings_dataset['itemID'].unique()) \n",
    "        \n",
    "        userId_lookup = StringLookup(vocabulary = ratings_dataset['userID'].unique(), mask_token = None )\n",
    "        asin_lookup = StringLookup(vocabulary = ratings_dataset['itemID'].unique(), mask_token = None )\n",
    "\n",
    "\n",
    "        item_input = Input(shape=[1],name='item-input'  , dtype= object)\n",
    "        user_input = Input(shape=[1], name='user-input' , dtype= object)\n",
    "\n",
    "        # MLP Embeddings\n",
    "        item_input1 = asin_lookup(item_input)#item_input\n",
    "        user_input1 = userId_lookup(user_input)#user_input\n",
    "\n",
    "        item_embedding_mlp = Embedding(n_items + 1, 16, name='item-embedding-mlp'  )(item_input1)\n",
    "        item_vec_mlp = Flatten(name='flatten-item-mlp')(item_embedding_mlp)\n",
    "\n",
    "        user_embedding_mlp = Embedding(n_users + 1, 16, name='user-embedding-mlp' )(user_input1)\n",
    "        user_vec_mlp = Flatten(name='flatten-user-mlp')(user_embedding_mlp)\n",
    "\n",
    "        # MF Embeddings\n",
    "        item_embedding_mf = Embedding(n_items + 1, 16, name='item-embedding-mf' , embeddings_regularizer='L2' )(item_input1)\n",
    "        item_vec_mf = Flatten(name='flatten-item-mf')(item_embedding_mf)\n",
    "\n",
    "        user_embedding_mf = Embedding(n_users + 1, 16, name='user-embedding-mf' , embeddings_regularizer='L2' )(user_input1)\n",
    "        user_vec_mf = Flatten(name='flatten-user-mf')(user_embedding_mf)\n",
    "\n",
    "        # MLP layers\n",
    "\n",
    "        concat = Concatenate()([item_vec_mlp, user_vec_mlp])\n",
    "        concat_dropout = Dropout(0.5)(concat)\n",
    "\n",
    "        fc_1 = Dense(128, name='fc-1', activation='relu')(concat_dropout)\n",
    "        fc_1_bn = BatchNormalization(name='batch-norm-1')(fc_1)\n",
    "        fc_1_dropout = Dropout(0.5)(fc_1_bn)\n",
    "\n",
    "        fc_2 = Dense(64, name='fc-2', activation='relu')(fc_1_dropout)\n",
    "        fc_2_bn = BatchNormalization(name='batch-norm-2')(fc_2)\n",
    "        fc_2_dropout = Dropout(0.5)(fc_2_bn)\n",
    "\n",
    "\n",
    "        # Prediction from both layers\n",
    "        pred_mlp = Dense(16, name='pred-mlp', activation='relu' )(fc_2_dropout)\n",
    "\n",
    "\n",
    "\n",
    "        pred_mf = Dot(name=\"Dot-Product\", axes=1)([item_vec_mf, user_vec_mf])\n",
    "        combine_mlp_mf = Concatenate()([pred_mf, pred_mlp])\n",
    "\n",
    "        # Final prediction\n",
    "        result = Dense(1, name='result', activation='relu' )(combine_mlp_mf)\n",
    "\n",
    "        model2 = Model([user_input, item_input], result)\n",
    "        model2.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), loss= tf.keras.losses.mean_absolute_error  , metrics= tf.keras.metrics.RootMeanSquaredError(\n",
    "            name=\"root_mean_squared_error\", dtype=None))\n",
    "        history = model2.fit([ratings_dataset.userID, ratings_dataset.itemID], ratings_dataset['rating'] , batch_size=64  , epochs=2,  verbose=1 )\n",
    "        model2.save(r'C:\\Users\\Sigma\\ecommerce\\embedding_model.h5')\n",
    "#         with open(r'C:\\Users\\Abdo\\Desktop\\last year\\project\\datasets\\last datataset isa\\embedding_model.pkl','wb') as f:\n",
    "#                 pickle.dump(model2, f)\n",
    "    \n",
    "    ratings_dataset2 = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\current_rating.csv\")\n",
    "    item_data = np.array(list(set(ratings_dataset2.itemID)))\n",
    "    user = np.array([userId for i in range(len(item_data))])\n",
    "    predictions = model2.predict([user, item_data])\n",
    "\n",
    "    predictions = np.array([ a[0] for a in predictions])\n",
    "\n",
    "    #recommended_indices = (-predictions).argsort()\n",
    "    #item_data[recommended_indices]\n",
    "    #     img_show_indices(recommended_indices[0:10])\n",
    "    a = ratings_dataset2[ratings_dataset2['userID'] == userId]['itemID']\n",
    "    #arr =  np.array(df2[df2['asin'].isin(item_data[recommended_indices]) & df2['asin'].isin(a)==0]['asin'])\n",
    "    d = {\"item\":item_data , \"pred\":predictions }\n",
    "    df = pd.DataFrame(d)\n",
    "    arr  = df.sort_values(by=\"pred\" , ascending=False)\n",
    "    arr = np.array(arr['item'])\n",
    "    \n",
    "    #arr = item_data[recommended_indices]\n",
    "    arr1 = []\n",
    "    for item in arr:\n",
    "        if item not in np.array(a):\n",
    "            arr1.append(item)\n",
    "    \n",
    "    arr1= arr1[0:numItems]\n",
    "    return arr1     \n",
    "def add_last_item(old_interaction, last_item_id, max_len=50):\n",
    "    new_seq_items = old_interaction['item_id_list'][-1]\n",
    "    if old_interaction['item_length'][-1].item() < max_len:\n",
    "        new_seq_items[old_interaction['item_length'][-1].item()] = last_item_id\n",
    "    else:\n",
    "        new_seq_items = torch.roll(new_seq_items, -1)\n",
    "        new_seq_items[-1] = last_item_id\n",
    "    return new_seq_items.view(1, len(new_seq_items))\n",
    "\n",
    "def predict_for_all_item(external_user_id, dataset, model ,config,train_data,valid_data,test_data , numItems = 5  ):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        uid_series = dataset.token2id(dataset.uid_field, [external_user_id])\n",
    "        index = np.isin(dataset[dataset.uid_field].numpy(), uid_series)\n",
    "        input_interaction = dataset[index]\n",
    "        test = {\n",
    "            'item_id_list': add_last_item(input_interaction, \n",
    "                                          input_interaction['item_id'][-1].item(), model.max_seq_length),\n",
    "            'item_length': torch.tensor(\n",
    "                [input_interaction['item_length'][-1].item() + 1\n",
    "                 if input_interaction['item_length'][-1].item() < model.max_seq_length else model.max_seq_length])\n",
    "        }\n",
    "        new_inter = Interaction(test)\n",
    "        new_inter = new_inter.to(config['device'])\n",
    "        new_scores = model.full_sort_predict(new_inter)\n",
    "        new_scores = new_scores.view(-1, test_data.dataset.item_num)\n",
    "        new_scores[:, 0] = -np.inf  # set scores of [pad] to -inf\n",
    "    return torch.topk(new_scores, numItems)# 11 to cancel the first one\n",
    "\n",
    "def bert(userId , numItems = 5 ):\n",
    "    df = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\both3.csv\")\n",
    "    df = df.sort_values(by=['timestamp'])\n",
    "    df1 = df.rename(columns={'userID': 'user_id:token', 'itemID': 'item_id:token', 'rating': 'rating:token' ,'timestamp': 'timestamp:float'})\n",
    "    df1.to_csv(r'C:\\Users\\Sigma\\ecommerce\\recbox_data_old.inter', index=False, sep='\\t')\n",
    "    \n",
    "    if os.path.exists('saved/BERT4Rec.pth'):\n",
    "        config, model, dataset, train_data, valid_data, test_data = load_data_and_model(model_file='saved/BERT4Rec.pth')\n",
    "        \n",
    "    else:\n",
    "        # train on old data\n",
    "        #########################################################################################################################\n",
    "        parameter_dict = {\n",
    "        'data_path': r'C:\\Users\\Sigma\\ecommerce\\recbox_data_old.inter',\n",
    "        'USER_ID_FIELD': 'user_id',\n",
    "        'ITEM_ID_FIELD': 'item_id',\n",
    "        'TIME_FIELD': 'timestamp',\n",
    "        'user_inter_num_interval': \"[ 0,100000]\",\n",
    "        'item_inter_num_interval': \"[ 0,100000]\",\n",
    "        #'additional_feat_suffix': ['item'],\n",
    "        'load_col': {'inter': ['user_id', 'item_id' , 'rating', 'timestamp'],\n",
    "                     #'item': ['item_id', 'item_emb']\n",
    "                    },\n",
    "\n",
    "        #'alias_of_item_id': ['item_id'],\n",
    "        #'preload_weight':{'item_id': 'item_emb'},\n",
    "\n",
    "        'unused_col':{'inter': ['rating'],\n",
    "                      #'item': ['item_id', 'item_emb']\n",
    "                     },\n",
    "        'metrics': ['Recall','NDCG','Hit'],\n",
    "        'topk'   : [1,5,10,20],\n",
    "        'train_neg_sample_args':None,\n",
    "        'seed':42,\n",
    "        'train_batch_size': 1024,\n",
    "        'eval_step': 1 ,                    # (int) The number of training epochs before an evaluation on the valid dataset.\n",
    "        'n_layers': 2 ,                     # (int) The number of transformer layers in transformer encoder.\n",
    "        'n_heads': 2  ,                    # (int) The number of attention heads for multi-head attention layer.\n",
    "        'hidden_size': 64 ,                 # (int) The number of features in the hidden state.\n",
    "        'inner_size': 256  ,               # (int) The inner hidden size in feed-forward layer.\n",
    "        'hidden_dropout_prob': 0.2 ,        # (float) The probability of an element to be zeroed.\n",
    "        'attn_dropout_prob': 0.2 ,          # (float) The probability of an attention score to be zeroed.\n",
    "        'hidden_act': 'gelu' ,             # (str) The activation function in feed-forward layer.\n",
    "        'layer_norm_eps': 1e-12 ,           # (float) A value added to the denominator for numerical stability.\n",
    "        'initializer_range': 0.02 ,         # (float) The standard deviation for normal initialization.\n",
    "        'mask_ratio': 0.2 ,                 # (float) The probability for a item replaced by MASK token.\n",
    "        'loss_type': 'CE' ,                # (str) The type of loss function.\n",
    "        'ft_ratio': 0.5   ,               # (float) The probability of generating fine-tuning samples\n",
    "        'neg_sampling': None,\n",
    "        'epochs': 1,\n",
    "        'eval_args': {\n",
    "            'split': {'RS': [1,0,0]},\n",
    "            'group_by': 'user',\n",
    "            'order': 'TO',\n",
    "            'mode': 'uni100'\n",
    "        }\n",
    "        }\n",
    "\n",
    "        config = Config(model= 'BERT4Rec', dataset='', config_dict=parameter_dict)\n",
    "\n",
    "        # init random seed\n",
    "        init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "        init_logger(config)\n",
    "        logger = getLogger()\n",
    "        # Create handlers\n",
    "        c_handler = logging.StreamHandler()\n",
    "        c_handler.setLevel(logging.INFO)\n",
    "        logger.addHandler(c_handler)\n",
    "        #########################################################################################################################\n",
    "        dataset = create_dataset(config )\n",
    "        train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "        model = BERT4Rec(config, train_data.dataset).to(config['device'])\n",
    "        trainer = Trainer(config, model)\n",
    "        # model training\n",
    "        best_valid_score, best_valid_result = trainer.fit(train_data)\n",
    "        #########################################################################################################################\n",
    "        config, model, dataset, train_data, valid_data, test_data = load_data_and_model(model_file='saved/BERT4Rec.pth')\n",
    "    \n",
    "    b = bert_update(userId , model)\n",
    "    old_new_condition100 = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\old&new_condition.inter\", sep='\\t')\n",
    "\n",
    "    if b or (userId in np.array(old_new_condition100['user_id:token']) ):\n",
    "        #external_user_ids = dataset.id2token(\n",
    "        #dataset.uid_field, list(range(dataset.user_num)))[1:]#fist element in array is 'PAD'(default of Recbole) ->remove it \n",
    "        config, model, dataset, train_data, valid_data, test_data = load_data_and_model(model_file='saved/BERT4Rec.pth')\n",
    "\n",
    "        # if you want to choose only one user\n",
    "        _, topk_iid_list = predict_for_all_item(userId, dataset, model ,config,train_data,valid_data,test_data , numItems )\n",
    "        last_topk_iid_list = topk_iid_list[-1]\n",
    "        external_item_list = dataset.id2token(dataset.iid_field, last_topk_iid_list.cpu()).tolist()\n",
    "    \n",
    "        return external_item_list\n",
    "    else:\n",
    "        arr = []\n",
    "        return arr\n",
    "\n",
    "def bert_update(userId , model):\n",
    "    new_all = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\new_all.csv\")\n",
    "    \n",
    "    if (len(new_all[new_all['userID']== userId]) >= 3):\n",
    "        new_all2 = new_all[new_all['userID']== userId]\n",
    "        new_all2 = new_all2.sort_values(by=['timestamp'])\n",
    "        new_all2 = new_all2.rename(columns={'userID': 'user_id:token', 'itemID': 'item_id:token', 'rating': 'rating:token' ,'timestamp': 'timestamp:float'})\n",
    "        new_all2.to_csv(r'C:\\Users\\Sigma\\ecommerce\\new_condition.inter', index=False, sep='\\t')\n",
    "        \n",
    "        new_all = new_all[new_all['userID']!= userId]\n",
    "        new_all.reset_index(drop=True)\n",
    "        #new_all = new_all.rename(columns={'userID': 'user_id:token', 'itemID': 'item_id:token', 'rating': 'rating:token' ,'timestamp': 'timestamp:float'})\n",
    "        new_all.to_csv(r\"C:\\Users\\Sigma\\ecommerce\\new_all.csv\" ,index=False)\n",
    "        \n",
    "        old_new_condition = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\old&new_condition.inter\", sep='\\t')\n",
    "        \n",
    "        # items in new_condition not in old&new_condition\n",
    "        \n",
    "        if (len(new_all2[~new_all2['item_id:token'].isin(old_new_condition['item_id:token'])]) > 0) :\n",
    "            model.update_embedding(len(new_all2[~new_all2['item_id:token'].isin(old_new_condition['item_id:token'])]))\n",
    "\n",
    "        \n",
    "        old_new_condition = old_new_condition.append(new_all2 )\n",
    "        old_new_condition.reset_index(drop = True)\n",
    "        old_new_condition = old_new_condition.sort_values(by=['timestamp:float'])\n",
    "        old_new_condition.to_csv(r'C:\\Users\\Sigma\\ecommerce\\old&new_condition.inter', index=False, sep='\\t')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "        #########################################################################################################################\n",
    "        parameter_dict = {\n",
    "        'data_path': r'C:\\Users\\Sigma\\ecommerce\\new_condition.inter', # put the new data that satisfy the condition\n",
    "        'USER_ID_FIELD': 'user_id',\n",
    "        'ITEM_ID_FIELD': 'item_id',\n",
    "        'TIME_FIELD': 'timestamp',\n",
    "        'user_inter_num_interval': \"[ 0,100000]\",\n",
    "        'item_inter_num_interval': \"[ 0,100000]\",\n",
    "        #'additional_feat_suffix': ['item'],\n",
    "        'load_col': {'inter': ['user_id', 'item_id' , 'rating', 'timestamp'],\n",
    "                     #'item': ['item_id', 'item_emb']\n",
    "                    },\n",
    "\n",
    "        #'alias_of_item_id': ['item_id'],\n",
    "        #'preload_weight':{'item_id': 'item_emb'},\n",
    "\n",
    "        'unused_col':{'inter': ['rating'],\n",
    "                      #'item': ['item_id', 'item_emb']\n",
    "                     },\n",
    "        'metrics': ['Recall','NDCG','Hit'],\n",
    "        'topk'   : [1,5,10,20],\n",
    "        'train_neg_sample_args':None,\n",
    "        'seed':42,\n",
    "        'train_batch_size': 1024 ,#10000000\n",
    "        'eval_step': 1 ,                    # (int) The number of training epochs before an evaluation on the valid dataset.\n",
    "        'n_layers': 2 ,                     # (int) The number of transformer layers in transformer encoder.\n",
    "        'n_heads': 2  ,                    # (int) The number of attention heads for multi-head attention layer.\n",
    "        'hidden_size': 64 ,                 # (int) The number of features in the hidden state.\n",
    "        'inner_size': 256  ,               # (int) The inner hidden size in feed-forward layer.\n",
    "        'hidden_dropout_prob': 0.2 ,        # (float) The probability of an element to be zeroed.\n",
    "        'attn_dropout_prob': 0.2 ,          # (float) The probability of an attention score to be zeroed.\n",
    "        'hidden_act': 'gelu' ,             # (str) The activation function in feed-forward layer.\n",
    "        'layer_norm_eps': 1e-12 ,           # (float) A value added to the denominator for numerical stability.\n",
    "        'initializer_range': 0.02 ,         # (float) The standard deviation for normal initialization.\n",
    "        'mask_ratio': 0.2 ,                 # (float) The probability for a item replaced by MASK token.\n",
    "        'loss_type': 'CE' ,                # (str) The type of loss function.\n",
    "        'ft_ratio': 0.5   ,               # (float) The probability of generating fine-tuning samples\n",
    "        'neg_sampling': None,\n",
    "        'epochs': 1,\n",
    "        'eval_args': {\n",
    "            'split': {'RS': [1,0,0]},\n",
    "            'group_by': 'user',\n",
    "            'order': 'TO',\n",
    "            'mode': 'uni100'\n",
    "        }\n",
    "        }\n",
    "\n",
    "        config = Config(model= 'BERT4Rec', dataset='', config_dict=parameter_dict)\n",
    "\n",
    "        # init random seed\n",
    "        init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "        init_logger(config)\n",
    "        logger = getLogger()\n",
    "        # Create handlers\n",
    "        c_handler = logging.StreamHandler()\n",
    "        c_handler.setLevel(logging.INFO)\n",
    "        logger.addHandler(c_handler)\n",
    "        #########################################################################################################################\n",
    "        dataset = create_dataset(config )\n",
    "        train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "        #########################################################################################################################\n",
    "\n",
    "        parameter_dict = {\n",
    "        'data_path': r'C:\\Users\\Sigma\\ecommerce\\old&new_condition.inter', # put the old data + new data that satisfy the condition\n",
    "        'USER_ID_FIELD': 'user_id',\n",
    "        'ITEM_ID_FIELD': 'item_id',\n",
    "        'TIME_FIELD': 'timestamp',\n",
    "        'user_inter_num_interval': \"[ 0,100000]\",\n",
    "        'item_inter_num_interval': \"[ 0,100000]\",\n",
    "        #'additional_feat_suffix': ['item'],\n",
    "        'load_col': {'inter': ['user_id', 'item_id' , 'rating', 'timestamp'],\n",
    "                     #'item': ['item_id', 'item_emb']\n",
    "                    },\n",
    "\n",
    "        #'alias_of_item_id': ['item_id'],\n",
    "        #'preload_weight':{'item_id': 'item_emb'},\n",
    "\n",
    "        'unused_col':{'inter': ['rating'],\n",
    "                      #'item': ['item_id', 'item_emb']\n",
    "                     },\n",
    "        'metrics': ['Recall','NDCG','Hit'],\n",
    "        'topk'   : [1,5,10,20],\n",
    "        'train_neg_sample_args':None,\n",
    "        'seed':42,\n",
    "        'train_batch_size': 4096,\n",
    "        'eval_step': 1 ,                    # (int) The number of training epochs before an evaluation on the valid dataset.\n",
    "        'n_layers': 2 ,                     # (int) The number of transformer layers in transformer encoder.\n",
    "        'n_heads': 2  ,                    # (int) The number of attention heads for multi-head attention layer.\n",
    "        'hidden_size': 64 ,                 # (int) The number of features in the hidden state.\n",
    "        'inner_size': 256  ,               # (int) The inner hidden size in feed-forward layer.\n",
    "        'hidden_dropout_prob': 0.2 ,        # (float) The probability of an element to be zeroed.\n",
    "        'attn_dropout_prob': 0.2 ,          # (float) The probability of an attention score to be zeroed.\n",
    "        'hidden_act': 'gelu' ,             # (str) The activation function in feed-forward layer.\n",
    "        'layer_norm_eps': 1e-12 ,           # (float) A value added to the denominator for numerical stability.\n",
    "        'initializer_range': 0.02 ,         # (float) The standard deviation for normal initialization.\n",
    "        'mask_ratio': 0.2 ,                 # (float) The probability for a item replaced by MASK token.\n",
    "        'loss_type': 'CE' ,                # (str) The type of loss function.\n",
    "        'ft_ratio': 0.5   ,               # (float) The probability of generating fine-tuning samples\n",
    "        'neg_sampling': None,\n",
    "        'epochs': 1,\n",
    "        'eval_args': {\n",
    "            'split': {'RS': [1,0,0]},\n",
    "            'group_by': 'user',\n",
    "            'order': 'TO',\n",
    "            'mode': 'uni100'\n",
    "        }\n",
    "        }\n",
    "\n",
    "        config1 = Config(model= 'BERT4Rec', dataset='', config_dict=parameter_dict)\n",
    "\n",
    "        # init random seed\n",
    "        init_seed(config1['seed'], config1['reproducibility'])\n",
    "\n",
    "        init_logger(config1)\n",
    "        logger = getLogger()\n",
    "        # Create handlers\n",
    "        c_handler = logging.StreamHandler()\n",
    "        c_handler.setLevel(logging.INFO)\n",
    "        logger.addHandler(c_handler)\n",
    "        #########################################################################################################################\n",
    "        trainer = Trainer(config1, model)\n",
    "        # model training\n",
    "        best_valid_score, best_valid_result = trainer.fit(train_data)\n",
    "        \n",
    "        new_all2 = new_all2[0:0]\n",
    "        new_all2.to_csv(r'C:\\Users\\Sigma\\ecommerce\\new_condition.inter', index=False, sep='\\t')\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def collabrative (user , numItems = 5   ):\n",
    "    from surprise.model_selection import train_test_split\n",
    "    ratings = pd.read_csv(r\"C:\\Users\\Sigma\\ecommerce\\current_rating.csv\")\n",
    "    ratings = pd.read_csv(ratingPath)\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(ratings[['userID', 'itemID', 'rating']], reader)\n",
    "    real_trainset = data.build_full_trainset()\n",
    "    if os.path.exists(r'C:\\Users\\Sigma\\ecommerce\\dump_file'):\n",
    "        file_name = os.path.expanduser(r\"C:\\Users\\Sigma\\ecommerce\\dump_file\")\n",
    "        _,svd = dump.load(file_name)\n",
    "       \n",
    "    else:\n",
    "    \n",
    "        \n",
    "        svd = SVD(n_factors= 30 , n_epochs= 20 , lr_all = 0.005 , reg_all = 0.02 , random_state=42  )\n",
    "        svd.fit(real_trainset)\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    real_testset = build_anti_testset(real_trainset   , user )\n",
    "    predictions = svd.test(real_testset)\n",
    "    top_n = get_top_n(predictions, n=numItems)\n",
    "    \n",
    "    \n",
    "    return (collaborative_recommendation(user , top_n , ratings))\n",
    "        \n",
    "def hybridRecomm ( userId = 0  ):\n",
    "    \n",
    "    \n",
    "    if  userId not in df3['userID'].values :\n",
    "        #give top_n model\n",
    "        a = topRecom( 30 )\n",
    "        _, idx = np.unique(a, return_index=True)\n",
    "        a = a[np.sort(idx)]\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    #calculate numbers of rating done by the user\n",
    "    \n",
    "    allUsersRatings = df3.groupby('userID')['userID'].count()\n",
    "    userRating = allUsersRatings[userId]\n",
    "    \n",
    "    #Determining users items \n",
    "    \n",
    "    userDataFrame = df3[df3['userID']== userId ]\n",
    "    userDataFrame = userDataFrame.sort_values(by=['rating'])\n",
    "    userAsinProducts = [] \n",
    "    for index, row in userDataFrame.iterrows():\n",
    "            userAsinProducts.append(row['itemID'])\n",
    "    \n",
    "    #calculating Avg of Description legnth of All products\n",
    "\n",
    "    sum1 = 0\n",
    "    for product in userAsinProducts:\n",
    "        productIndex = (df2[df2['asin']== product ]).index[0]\n",
    "        descriptionLen = len(df2[df2['asin']==product]['feature'][productIndex])\n",
    "        sum1 = sum1 + descriptionLen\n",
    "    avg = sum1 / userRating\n",
    "    \n",
    "    \n",
    "    if (userRating <= 3 ) :\n",
    "        if(avg <= 500 ):\n",
    "            recomendedProducts = [] \n",
    "            numOfContentBased = math.ceil(10/ userRating)\n",
    "            for product in userAsinProducts:\n",
    "#                 arr1 = contentBasedDescription(product, numOfContentBased )\n",
    "                \n",
    "#                 arr2 = contentBasedBrand(product,numOfContentBased)\n",
    "                arr1 = contentBased(product, numOfContentBased)\n",
    "                arr3 = imgRecomm(product,numOfContentBased)\n",
    "                recomendedProducts = np.concatenate(  (recomendedProducts ,arr1  , arr3) , axis = None ) \n",
    "            recomendedProducts = np.concatenate((recomendedProducts, collabrative(userId,5)  , topRecom( 10 )) , axis = None  )\n",
    "            \n",
    "            \n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "        else:\n",
    "            recomendedProducts = [] \n",
    "            numOfContentBased = math.ceil(15/ userRating)\n",
    "            for product in userAsinProducts:\n",
    "#                 arr1 = contentBasedDescription(product, numOfContentBased )\n",
    "                \n",
    "#                 arr2 = contentBasedBrand(product,numOfContentBased)\n",
    "                arr1 = contentBased(product, numOfContentBased)\n",
    "                arr3 = imgRecomm(product,numOfContentBased)\n",
    "                recomendedProducts = np.concatenate( (recomendedProducts ,arr1  , arr3) ,axis = None  )\n",
    "            \n",
    "            recomendedProducts =np.concatenate((recomendedProducts, collabrative(userId,5)) , axis = None)\n",
    "            \n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "    else :\n",
    "        if(avg <= 400 ):\n",
    "            recomendedProducts = [] \n",
    "            if(userRating <= 5 ):\n",
    "                numOfContentBased = math.ceil(5/ userRating)\n",
    "            else :\n",
    "                numOfContentBased = 2\n",
    "            x = 0\n",
    "            for product in userAsinProducts:\n",
    "                if (x == 5 ): \n",
    "                    break \n",
    "                x = x+1 \n",
    "                \n",
    "\n",
    "                arr1 = contentBased(product, numOfContentBased)\n",
    "                arr3 = imgRecomm(product,numOfContentBased)\n",
    "                recomendedProducts = np.concatenate( (recomendedProducts ,arr1 , arr3) , axis = None)\n",
    "            if (len(recomendedProducts) > 20):\n",
    "                np.random.shuffle(recomendedProducts)\n",
    "                recomendedProducts = recomendedProducts[0:20]\n",
    "            recomendedProducts = np.concatenate((recomendedProducts ,bert(userId,15) , embedding (userId ,5)) , axis = None )\n",
    "            \n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "        else:\n",
    "            recomendedProducts = [] \n",
    "            if(userRating <= 5 ):\n",
    "                numOfContentBased = round(5/ userRating)\n",
    "            else :\n",
    "                numOfContentBased = 2 \n",
    "            x = 0\n",
    "            for product in userAsinProducts:\n",
    "                if (x == 5 ): \n",
    "                    break \n",
    "                x = x+1 \n",
    "                \n",
    "\n",
    "                arr1 = contentBased(product, numOfContentBased)\n",
    "                \n",
    "                recomendedProducts = np.concatenate( (recomendedProducts , arr1 ) , axis = None )\n",
    "            \n",
    "            if (len(recomendedProducts) > 20):\n",
    "                np.random.shuffle(recomendedProducts)\n",
    "                recomendedProducts = recomendedProducts[0:20]\n",
    "            recomendedProducts = np.concatenate((recomendedProducts , bert(userId,10) , embedding (userId ,5)) , axis = None )\n",
    "\n",
    "            a = recomendedProducts\n",
    "            _, idx = np.unique(a, return_index=True)\n",
    "            a = a[np.sort(idx)]\n",
    "            \n",
    "            return a\n",
    "        \n",
    "            \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a01287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5623528",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10100\\32546335.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdffcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f24d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb43858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hima \n",
    "#simple chat bot \n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "with open('Intent3.json') as f:\n",
    "    intents = json.load(f)\n",
    "def preprocessing(line):\n",
    "    line = re.sub(r'[^a-zA-z.?!\\']', ' ', line)\n",
    "    line = re.sub(r'[ ]+', ' ', line)\n",
    "    line = line.lower()\n",
    "    return line\n",
    "\n",
    "inputs, targets = [], []\n",
    "classes = []\n",
    "intent_doc = {}\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    if intent['intent'] not in classes:\n",
    "        classes.append(intent['intent'])\n",
    "    if intent['intent'] not in intent_doc:\n",
    "        intent_doc[intent['intent']] = []\n",
    "        \n",
    "    for text in intent['text']:\n",
    "        inputs.append(preprocessing(text))\n",
    "        targets.append(intent['intent'])\n",
    "        \n",
    "    for response in intent['responses']:\n",
    "        intent_doc[intent['intent']].append(response)\n",
    "\n",
    "def tokenize_data(input_list):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "    \n",
    "    tokenizer.fit_on_texts(input_list)\n",
    "    \n",
    "    input_seq = tokenizer.texts_to_sequences(input_list)\n",
    "\n",
    "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, padding='pre')\n",
    "    \n",
    "    return tokenizer, input_seq\n",
    "\n",
    "# preprocess input data\n",
    "tokenizer, input_tensor = tokenize_data(inputs)\n",
    "def create_categorical_target(targets):\n",
    "    word={}\n",
    "    categorical_target=[]\n",
    "    counter=0\n",
    "    for trg in targets:\n",
    "        if trg not in word:\n",
    "            word[trg]=counter\n",
    "            counter+=1\n",
    "        categorical_target.append(word[trg])\n",
    "    \n",
    "    categorical_tensor = tf.keras.utils.to_categorical(categorical_target, num_classes=len(word), dtype='int32')\n",
    "    return categorical_tensor, dict((v,k) for k, v in word.items())\n",
    "\n",
    "# preprocess output data\n",
    "target_tensor, trg_index_word = create_categorical_target(targets)\n",
    "\n",
    "model = keras.models.load_model('model.h5')\n",
    "\n",
    "def response(sentence):\n",
    "    sent_seq = []\n",
    "    doc = nlp(repr(sentence))\n",
    "    \n",
    "    # split the input sentences into words\n",
    "    for token in doc:\n",
    "        if token.text in tokenizer.word_index:\n",
    "            sent_seq.append(tokenizer.word_index[token.text])\n",
    "\n",
    "        # handle the unknown words error\n",
    "        else:\n",
    "            sent_seq.append(tokenizer.word_index['<unk>'])\n",
    "\n",
    "    sent_seq = tf.expand_dims(sent_seq, 0)\n",
    "    # predict the category of input sentences\n",
    "    pred = model(sent_seq)\n",
    "    pred_class = np.argmax(pred.numpy(), axis=1)\n",
    "    \n",
    "    # choice a random response for predicted sentence\n",
    "    if np.max(pred.numpy(), axis=1) > 0.1:\n",
    "        return random.choice(intent_doc[trg_index_word[pred_class[0]]]), trg_index_word[pred_class[0]]\n",
    "    else:\n",
    "        #return  \"No_answer\", \"cannot identify the question\"\n",
    "        return random.choice(intent_doc[trg_index_word[list(trg_index_word.keys())\n",
    "      [list(trg_index_word.values()).index('noanswer')]]]), trg_index_word[list(trg_index_word.keys())[list(trg_index_word.values()).index('noanswer')]]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3966848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sigma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bs4 import BeautifulSoup\n",
    "import string \n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"new.csv\")\n",
    "data=data[[\"question\",\"answer\"]]\n",
    "data\n",
    "\n",
    "\n",
    "\n",
    "def lower_text(text):\n",
    "    text = \" \".join(i.lower() for i in text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def stemmer(text):\n",
    "    ss=SnowballStemmer(language='english')\n",
    "    text= ' '.join([ss.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_noise(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(\"\\[[^]]*\\]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    tokens = text.split()\n",
    "    re_punc = re.compile(\"[%s]\"%re.escape(string.punctuation))\n",
    "    tokens = [re_punc.sub(\"\", w) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stop(text):\n",
    "    tokens = text.split()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    tokens_without_sw = [word for word in tokens if not word in all_stopwords]\n",
    "    text = \" \".join(tokens_without_sw)\n",
    "    return text\n",
    "\n",
    "def preprocess_data(text):\n",
    "    text = lower_text(text)\n",
    "    text = remove_noise(text)\n",
    "    text = remove_stop(text)\n",
    "    text = remove_punc(text)\n",
    "    text = stemmer(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "with open(r'FAQs_embeddings.pkl','rb') as f:\n",
    "    FAQs_embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FAQs_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chatMe(message) :\n",
    "    FAQs_test = pd.Series([message]).apply(preprocess_data)# put your input here\n",
    "\n",
    "    FAQs_test_embeddings = model.encode(FAQs_test)\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    similarity_score = cosine_similarity(FAQs_test_embeddings, FAQs_embeddings)\n",
    "    similarity_score\n",
    "\n",
    "\n",
    "\n",
    "    similarity_score.max(axis=1)\n",
    "\n",
    "    index = similarity_score.argmax(axis=1)\n",
    "\n",
    "    df2 = []\n",
    "    for i in index:\n",
    "        df2.append(data.answer[i])\n",
    "\n",
    "    df1 = pd.DataFrame({\"FAQs_test\":FAQs_test})\n",
    "    df1\n",
    "\n",
    "    df2 = pd.DataFrame(df2, columns=[\"FAQs_Answer\"])\n",
    "    df2\n",
    "\n",
    "    result = pd.concat([df1, df2], axis=1, join='inner')\n",
    "\n",
    "    return result.FAQs_Answer[0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afade3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:80/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/Jun/2023 23:48:35] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Jun/2023 23:48:41] \"POST /customerService HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from flask import Flask, flash, request, redirect, url_for\n",
    "from werkzeug.utils import secure_filename\n",
    "from flask import Flask\n",
    "from flask_restful import Resource , Api\n",
    "from flask import request, render_template\n",
    "from flask_cors import CORS\n",
    "import json as json\n",
    "import csv\n",
    "\n",
    "UPLOAD_FOLDER = \"\"\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'}\n",
    "\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/recomenByUser', methods =[\"POST\"])\n",
    "def recomend():\n",
    "    data = request.form['data']\n",
    "    data = str(data)\n",
    "    x = str(bert(data,20))\n",
    "    return (x)\n",
    "@app.route('/recomenByAsin', methods =[\"POST\"])\n",
    "def recomend2():\n",
    "    data = request.form['data']\n",
    "    data = str(data)\n",
    "    x = str(hybridRecomm(data))\n",
    "    return (x)\n",
    "@app.route('/sentiment', methods =[\"POST\"])\n",
    "def sentiment():\n",
    "    data = request.form['data']\n",
    "    data = str(data)\n",
    "    x = str(getSentiment(data))\n",
    "    # old_stdout = sys.stdout # backup current stdout\n",
    "    # sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    #     x = str(getSentiment(data))\n",
    "\n",
    "    # sys.stdout = old_stdout # reset old stdout \n",
    "\n",
    "    return x\n",
    "@app.route('/chat', methods =[\"POST\"])\n",
    "def chat():\n",
    "    data = request.form['data']\n",
    "    data = str(data)\n",
    "    res, typ = response(data)\n",
    "    result = res + '  \"'+typ+'\"'\n",
    "    result\n",
    "    return result\n",
    "\n",
    "@app.route('/customerService', methods =[\"POST\"])\n",
    "def chatService():\n",
    "    data = request.form['data']\n",
    "    data = str(chatMe(data))\n",
    "    return data\n",
    "\n",
    "@app.route('/upload', methods =[\"POST\"])\n",
    "def imgsearch():\n",
    "        if 'file' not in request.files:\n",
    "            flash('No file part')\n",
    "            return redirect(request.url)\n",
    "        file = request.files['file']\n",
    "        # If the user does not select a file, the browser submits an\n",
    "        # empty file without a filename.\n",
    "        if file.filename == '':\n",
    "            flash('No selected file')\n",
    "            return redirect(request.url)\n",
    "        if file and allowed_file(file.filename):\n",
    "            filename = secure_filename(file.filename)\n",
    "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "            x = get_upload_img(model_img,filename).reshape(1,-1)\n",
    "#             imgRecomm(asin , numItems = 5  )\n",
    "            cosine_sim_new3 = cosine_similarity(x, df_embs)\n",
    "            imgarr = transformer ('none', cosine_sim_new3 , 10)\n",
    "            imgarr = str( imgarr)\n",
    "            if os.path.isfile(filename):\n",
    "                os.remove(filename)\n",
    "            \n",
    "            \n",
    "            \n",
    "            return imgarr\n",
    "        return 'done'\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tapp.run(port=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c0b24ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mine have s 9 inch inseam and are about 2 inches above the knee (they're a little baggy)\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatMe(\"short\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
